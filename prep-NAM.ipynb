{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb2f77d-95b9-4a56-93b2-22661ff757c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def NAM_filter_and_match_dims(input_dir, output_dir, variables):\n",
    "    # List all files in input directory\n",
    "    file_list = [(os.path.join(input_dir, file), file) for file in os.listdir(input_dir) if file.endswith('.nc')]\n",
    "\n",
    "    for file_path, file_name in file_list:  # Unpack the tuple\n",
    "        try:\n",
    "            # Step 1: Filter the variables using NAM_filter_vars logic\n",
    "            with xr.open_dataset(file_path) as ds:\n",
    "                existing_vars = {var: ds[var] for var in variables.keys() if var in ds}\n",
    "                if not existing_vars:\n",
    "                    print(f\"No matching variables found in {file_name}.\")\n",
    "                    continue\n",
    "\n",
    "                # Filter and rename variables\n",
    "                ds_filtered = xr.Dataset(existing_vars).rename(variables)\n",
    "\n",
    "                for var in ds_filtered.data_vars:\n",
    "                    if 'time' in ds_filtered[var].dims:\n",
    "                        dims = ('time',) + tuple(d for d in ds_filtered[var].dims if d != 'time')\n",
    "                        ds_filtered[var] = ds_filtered[var].transpose(*dims)\n",
    "\n",
    "                for orig_var, new_var in variables.items():\n",
    "                    if orig_var in ds:\n",
    "                        ds_filtered[new_var].attrs = ds[orig_var].attrs\n",
    "\n",
    "                ds_filtered.attrs = ds.attrs\n",
    "\n",
    "                # Change longitude values to be in degrees west\n",
    "                if 'longitude' in ds_filtered:\n",
    "                    lon = ds_filtered['longitude'].values\n",
    "                    lon = np.where(lon > 180, lon - 360, lon)\n",
    "                    ds_filtered['longitude'].values = lon\n",
    "                    ds_filtered['longitude'].attrs['units'] = 'degrees_west'\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_name}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Step 2: Interpolate variables using NAM_match_dims logic\n",
    "        latitudes = ds_filtered['latitude'].values  # Shape: (67, 71)\n",
    "        longitudes = ds_filtered['longitude'].values  # Shape: (67, 71)\n",
    "        time = ds_filtered['time']\n",
    "        \n",
    "        # Save all the attributes for each variable\n",
    "        lat_attrs = ds_filtered['latitude'].attrs\n",
    "        lon_attrs = ds_filtered['longitude'].attrs\n",
    "        time_attrs = ds_filtered['time'].attrs\n",
    "        \n",
    "        # Flatten latitude and longitude for interpolation\n",
    "        points = np.array([(lon, lat) for lat_row, lon_row in zip(latitudes, longitudes) for lat, lon in zip(lat_row, lon_row)])\n",
    "        \n",
    "        # Define the new latitude and longitude grid\n",
    "        new_latitudes = np.linspace(np.min(latitudes), np.max(latitudes), num=67)\n",
    "        new_longitudes = np.linspace(np.min(longitudes), np.max(longitudes), num=67)\n",
    "        \n",
    "        # Create new meshgrid\n",
    "        new_lon_grid, new_lat_grid = np.meshgrid(new_longitudes, new_latitudes)\n",
    "        \n",
    "        new_vars = {}\n",
    "        \n",
    "        for var_name in ds_filtered.data_vars:\n",
    "            var = ds_filtered[var_name]\n",
    "            new_var_list = []\n",
    "            \n",
    "            for t in range(len(var.time)):\n",
    "                weather_variable = var.values[t, :, :]  # Shape (67, 71)\n",
    "                \n",
    "                # Flatten the weather variable data\n",
    "                values = weather_variable.flatten()\n",
    "                \n",
    "                # Interpolate the data onto the new grid\n",
    "                new_weather_variable = griddata(points, values, (new_lon_grid, new_lat_grid), method='linear')\n",
    "                \n",
    "                # Append the interpolated data for the current time step\n",
    "                new_var_list.append(new_weather_variable)\n",
    "            \n",
    "            # Stack the new variables along the time dimension\n",
    "            new_vars[var_name] = (['time', 'latitude', 'longitude'], np.stack(new_var_list))\n",
    "        \n",
    "        # Create a new xarray Dataset\n",
    "        new_ds = xr.Dataset(\n",
    "            new_vars, coords={'latitude': new_latitudes,\n",
    "                              'longitude': new_longitudes,\n",
    "                              'time': time.values})\n",
    "        \n",
    "        # Add the original variable attributes\n",
    "        new_ds['time'].attrs.update(time_attrs)\n",
    "        new_ds['latitude'].attrs.update(lat_attrs)\n",
    "        new_ds['longitude'].attrs.update(lon_attrs)\n",
    "\n",
    "        # Add global attributes\n",
    "        new_ds.attrs.update(ds_filtered.attrs)\n",
    "\n",
    "        #Saving files\n",
    "        filename = os.path.basename(file_name)\n",
    "        output_file = os.path.join(output_dir, filename)\n",
    "        print(f\"Saving file to: {output_file}\")  # Print the output file path\n",
    "        new_ds.to_netcdf(output_file)\n",
    "#-------------------------------------------------------------------------\n",
    "def main():\n",
    "    input_dir = \"/D4/data/gvaillant/NAM-2019-netcdf/07\"\n",
    "    output_dir = \"/D3/data/gvaillant/NAM/2019/intermediate/07\"\n",
    "    variables = {'TMP_2maboveground': 'T2'}\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    \n",
    "    # Assuming 'T2' is the dictionary of original variable names and their new names\n",
    "    NAM_filter_and_match_dims(input_dir, output_dir, variables)\n",
    "    \n",
    "    print(\"Done processing stage1 NAM files!\")\n",
    "\n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b252ac-fc70-4fc7-8460-79069ac2f9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NAM_spatial_filter(input_dir, output_dir):\n",
    "    \n",
    "    #Bounds to cover Manhattan (extend a bit higher?)\n",
    "    min_lat = 40.533801\n",
    "    max_lat = 40.955109\n",
    "    min_lon = -74.131557\n",
    "    max_lon = -73.762832\n",
    "    \n",
    "\n",
    "    for file_name in os.listdir(input_dir):\n",
    "        print(f\"Processing file: {file_name}\")\n",
    "        if file_name.endswith('.nc'):\n",
    "\n",
    "            file_path = os.path.join(input_dir, file_name)\n",
    "            dataset = xr.open_dataset(file_path)\n",
    "            \n",
    "            #Extract latitude and longitude variables\n",
    "            lat_var = 'latitude'\n",
    "            lon_var = 'longitude'\n",
    "            lat = dataset[lat_var].values\n",
    "            lon = dataset[lon_var].values\n",
    "            \n",
    "            #Filter the data based off of the spatial bounds\n",
    "            filtered_data = dataset.where(\n",
    "                (dataset[lat_var] >= min_lat) & (dataset[lat_var] <= max_lat) &\n",
    "                (dataset[lon_var] >= min_lon) & (dataset[lon_var] <= max_lon), drop=True)\n",
    "\n",
    "            output_file_path = os.path.join(output_dir, file_name)\n",
    "            filtered_data.to_netcdf(output_file_path)\n",
    "\n",
    "def main():\n",
    "    input_dir = \"/D4/data/gvaillant/NAM/2019/intermediate/06\"\n",
    "    output_dir = \"/D4/data/gvaillant/NAM/2019/NYC-final/06\"\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    \n",
    "    NAM_spatial_filter(input_dir, output_dir)\n",
    "    \n",
    "    print(\"Done processing stage1 NAM files!\")\n",
    "\n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dcc1be47-32a4-4395-80c0-2745d6a3c2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: /D3/data/gvaillant/NAM/2019/final/trial-run/train\n",
      "Saved combined dataset to: /D3/data/gvaillant/NAM/2019/final/trial-run/train/NAM_final_01-02.nc\n",
      "Done combining the first two months of NAM files!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "\n",
    "def NAM_combine_seq(input_dirs, output_dir):\n",
    "    # Given two directories, combine files sequentially\n",
    "    combined_datasets = []\n",
    "    \n",
    "    for input_dir in input_dirs:\n",
    "        nc_files = sorted([os.path.join(input_dir, file) for file in os.listdir(input_dir) if file.endswith('.nc')])\n",
    "        datasets = [xr.open_dataset(nc_file) for nc_file in nc_files]\n",
    "        combined_datasets.extend(datasets)  # Add datasets sequentially\n",
    "\n",
    "    # Concatenate along the time dimension\n",
    "    combined_dataset = xr.concat(combined_datasets, dim='time')\n",
    "    \n",
    "    # Generate the output filename based on the directories' month identifiers\n",
    "    month_range = \"-\".join([os.path.basename(dir) for dir in input_dirs])\n",
    "    output_file_name = f'NAM_final_{month_range}.nc'\n",
    "    output_file_path = os.path.join(output_dir, output_file_name)\n",
    "    \n",
    "    # Save the combined dataset\n",
    "    combined_dataset.to_netcdf(output_file_path, encoding={'time': {'units': 'hours since 2019-1-1'}})\n",
    "    print(f\"Saved combined dataset to: {output_file_path}\")\n",
    "\n",
    "def main():\n",
    "\n",
    "    input_dirs = [\n",
    "        \"/D4/data/gvaillant/NAM/2019/NYC-final/03\"\n",
    "    ]\n",
    "    \n",
    "    output_dir = \"/D4/data/gvaillant/NAM/2019/final/trial-run/test\"\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    \n",
    "    NAM_combine_seq(input_dirs, output_dir)\n",
    "    \n",
    "    print(\"Done combining the first two months of NAM files!\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e28cf071-8e7d-4b6b-ac03-bb23ee299cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: /D3/data/gvaillant/NAM/2019/final/trial-run/test\n",
      "Saved combined dataset to: /D3/data/gvaillant/NAM/2019/final/trial-run/test/NAM_final_03.nc\n",
      "Done combining the first half of the NAM files!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "\n",
    "def NAM_val_test(input_dirs, output_dir):\n",
    "    # Given two directories, combine files sequentially\n",
    "    combined_datasets = []\n",
    "    \n",
    "    for input_dir in input_dirs:\n",
    "        # Get sorted list of .nc files\n",
    "        nc_files = sorted([os.path.join(input_dir, file) for file in os.listdir(input_dir) if file.endswith('.nc')])\n",
    "        \n",
    "        # Take the first half of the files\n",
    "        half_length = len(nc_files) // 2\n",
    "        #selected_files = nc_files[:half_length]  # Select first half of the sorted files\n",
    "        selected_files = nc_files[half_length:] #Select second half of the sorted files\n",
    "        # Open the selected files\n",
    "        datasets = [xr.open_dataset(nc_file) for nc_file in selected_files]\n",
    "        combined_datasets.extend(datasets)  # Add datasets sequentially\n",
    "\n",
    "    # Concatenate along the time dimension\n",
    "    combined_dataset = xr.concat(combined_datasets, dim='time')\n",
    "    \n",
    "    # Generate the output filename based on the directories' month identifiers\n",
    "    month_range = \"-\".join([os.path.basename(dir) for dir in input_dirs])\n",
    "    output_file_name = f'NAM_final_{month_range}.nc'\n",
    "    output_file_path = os.path.join(output_dir, output_file_name)\n",
    "    \n",
    "    # Save the combined dataset\n",
    "    combined_dataset.to_netcdf(output_file_path, encoding={'time': {'units': 'hours since 2019-1-1'}})\n",
    "    print(f\"Saved combined dataset to: {output_file_path}\")\n",
    "\n",
    "def main():\n",
    "    input_dirs = [\n",
    "        \"/D4/data/gvaillant/NAM/2019/NYC-final/03\"\n",
    "    ]\n",
    "    \n",
    "    output_dir = \"/D4/data/gvaillant/NAM/2019/final/trial-run/test\"\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    \n",
    "    NAM_val_test(input_dirs, output_dir)\n",
    "    \n",
    "    print(\"Done combining the first half of the NAM files!\")\n",
    "\n",
    "#main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
