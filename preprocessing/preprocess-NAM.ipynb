{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0121312-b916-42c2-8a5b-aaa5c649dfd9",
   "metadata": {},
   "source": [
    "# Preprocessing NAM-NMM Dataset\n",
    "\n",
    "\n",
    "Pre-req:\n",
    "Download NAM-NMM files from BNL's remote servers\n",
    "(using command terminal).\n",
    "\n",
    "## Step 1:\n",
    "\n",
    "Use the NAM_filter_vars function to filter the dataset down to the 4 variables of interest and change the names of the variables to match uWRF naming: \n",
    "\n",
    "* TMP_2maboveground -> T2\n",
    "* UGRD_10maboveground -> U10\n",
    "* VGRD_10maboveground -> V10\n",
    "* PRES_surface -> PSFC\n",
    "\n",
    "This function also changes the way the longitude values are measured.\n",
    "\n",
    "## Step 2:\n",
    "Change the dimensions of the dataset from y,x to latitude, longitude and keep them a consistent size\n",
    "\n",
    "## Step 3: \n",
    "Filter spatially to only include area covering Manhattan\n",
    "\n",
    "min_lat = 40.57384924257281\n",
    "\n",
    "max_lat = 40.92\n",
    "\n",
    "min_lon = -74.0481110602903\n",
    "\n",
    "max_lon = -73.84627819243957\n",
    "\n",
    "## Step 4:\n",
    "Combine each day of data into a sequential format\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "35889eb1-f3c8-48ac-a452-1a6d5813aaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4\n",
    "import xarray as xarray\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from scipy.interpolate import griddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "ef0d2a5a-0a4b-4058-957b-6fc075d3015b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NAM_filter_vars(input_dir, output_dir, variables):\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    input_files = glob.glob(os.path.join(input_dir, '*.nc')) #Take all the .nc files from input_dir\n",
    "\n",
    "    for file in input_files:\n",
    "        try:\n",
    "            with xr.open_dataset(file) as ds:\n",
    "                existing_vars = {var: ds[var] for var in variables.keys() if var in ds}\n",
    "                if not existing_vars:\n",
    "                    print(f\"No matching variables found in file {file}.\")\n",
    "                    continue\n",
    "\n",
    "                #Filter the variables\n",
    "                ds_filtered = xr.Dataset(existing_vars)\n",
    "                \n",
    "                #Rename the variables\n",
    "                ds_filtered = ds_filtered.rename(variables)\n",
    "\n",
    "                for var in ds_filtered.data_vars:\n",
    "                    if 'time' in ds_filtered[var].dims:\n",
    "                        dims = ('time',) + tuple(d for d in ds_filtered[var].dims if d != 'time')\n",
    "                        ds_filtered[var] = ds_filtered[var].transpose(*dims)\n",
    "                \n",
    "                \n",
    "                for orig_var, new_var in variables.items():\n",
    "                    if orig_var in ds:\n",
    "                        ds_filtered[new_var].attrs = ds[orig_var].attrs\n",
    "        \n",
    "                ds_filtered.attrs = ds.attrs\n",
    "                \n",
    "                #Change longitude values\n",
    "                if 'longitude' in ds_filtered:\n",
    "                    lon = ds_filtered['longitude'].values\n",
    "                    lon = np.where(lon > 180, lon - 360, lon)\n",
    "                    ds_filtered['longitude'].values = lon\n",
    "                    ds_filtered['longitude'].attrs['units'] = 'degrees_west'\n",
    "\n",
    "                filename = os.path.basename(file)\n",
    "                output_file = os.path.join(output_dir, filename)\n",
    "                ds_filtered.to_netcdf(output_file)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "    print('Done filtering files!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "148ad417-f8f4-471a-9696-10875cfdd951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NAM_match_dims(input_dir, output_dir):\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for file_name in os.listdir(input_dir):\n",
    "        if file_name.endswith('.nc'):\n",
    "            \n",
    "            input_file = os.path.join(input_dir, file_name)\n",
    "            \n",
    "            ds = xr.open_dataset(input_file)\n",
    "            \n",
    "            latitudes = ds['latitude'].values  # Shape: (67, 71)\n",
    "            longitudes = ds['longitude'].values  # Shape: (67, 71)\n",
    "            time = ds['time']\n",
    "            \n",
    "            #Save all the attributes for each variable\n",
    "            lat_attrs = ds['latitude'].attrs\n",
    "            lon_attrs = ds['longitude'].attrs\n",
    "            time_attrs = ds['time'].attrs \n",
    "            tmp_attrs = ds['T2'].attrs\n",
    "            u_attrs = ds['U10'].attrs\n",
    "            v_attrs = ds['V10'].attrs\n",
    "            psfc_attrs = ds['PSFC'].attrs\n",
    "            \n",
    "            #Flatten latitude and longitude for interpolation\n",
    "            points = np.array([(lon, lat) for lat_row, lon_row in zip(latitudes, longitudes) for lat, lon in zip(lat_row, lon_row)])\n",
    "            \n",
    "            #Define the new latitude and longitude grid\n",
    "            new_latitudes = np.linspace(np.min(latitudes), np.max(latitudes), num=67)\n",
    "            new_longitudes = np.linspace(np.min(longitudes), np.max(longitudes), num=67)  #**CHANGED TO 67**\n",
    "            \n",
    "            #Create new meshgrid\n",
    "            new_lon_grid, new_lat_grid = np.meshgrid(new_longitudes, new_latitudes)\n",
    "            \n",
    "            new_vars = {}\n",
    "            \n",
    "            for var_name in ds.data_vars:\n",
    "                \n",
    "                var = ds[var_name]\n",
    "                new_var_list = []\n",
    "                \n",
    "                for t in range(len(var.time)):\n",
    "                    weather_variable = var.values[t, :, :]  # Shape (67, 71)\n",
    "                    \n",
    "                    #Flatten the weather variable data\n",
    "                    values = weather_variable.flatten()\n",
    "                    \n",
    "                    #Interpolate the data onto the new grid\n",
    "                    new_weather_variable = griddata(points, values, (new_lon_grid, new_lat_grid), method='linear')\n",
    "                    \n",
    "                    #Append the interpolated data for the current time step\n",
    "                    new_var_list.append(new_weather_variable)\n",
    "                \n",
    "                #Stack the new variables along the time dimension\n",
    "                new_vars[var_name] = (['time', 'latitude', 'longitude'], np.stack(new_var_list))\n",
    "            \n",
    "            #Create a new xarray Dataset\n",
    "            new_ds = xr.Dataset(\n",
    "                new_vars, coords={'latitude': new_latitudes,\n",
    "                    'longitude': new_longitudes,\n",
    "                    'time': time.values})\n",
    "            \n",
    "            #Add the original variable attributes\n",
    "            new_ds['time'].attrs.update(time_attrs)\n",
    "            new_ds['latitude'].attrs.update(lat_attrs)\n",
    "            new_ds['longitude'].attrs.update(lon_attrs)\n",
    "            new_ds['T2'].attrs.update(tmp_attrs)\n",
    "            new_ds['U10'].attrs.update(u_attrs)\n",
    "            new_ds['V10'].attrs.update(v_attrs)\n",
    "            new_ds['PSFC'].attrs.update(psfc_attrs)\n",
    "            \n",
    "            #Add global attributes\n",
    "            new_ds.attrs.update(ds.attrs)\n",
    "            \n",
    "            output_file = os.path.join(output_dir, file_name)\n",
    "            new_ds.to_netcdf(output_file)\n",
    "            \n",
    "    print('Done filtering files!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "69ea1efb-f6ed-404b-af5a-77a13681720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NAM_spatial_filter(input_dir, output_dir):\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    #Bounds to cover Manhattan (extend a bit higher?)\n",
    "    min_lat = 40.57384924257281\n",
    "    max_lat = 40.92\n",
    "    min_lon = -74.0481110602903\n",
    "    max_lon = -73.84627819243957\n",
    "    \n",
    "\n",
    "    for file_name in os.listdir(input_dir):\n",
    "        if file_name.endswith('.nc'):\n",
    "\n",
    "            file_path = os.path.join(input_dir, file_name)\n",
    "            dataset = xr.open_dataset(file_path)\n",
    "            \n",
    "            #Extract latitude and longitude variables\n",
    "            lat_var = 'latitude'\n",
    "            lon_var = 'longitude'\n",
    "            lat = dataset[lat_var].values\n",
    "            lon = dataset[lon_var].values\n",
    "            \n",
    "            #Filter the data based off of the spatial bounds\n",
    "            filtered_data = dataset.where(\n",
    "                (dataset[lat_var] >= min_lat) & (dataset[lat_var] <= max_lat) &\n",
    "                (dataset[lon_var] >= min_lon) & (dataset[lon_var] <= max_lon), drop=True)\n",
    "\n",
    "            output_file_path = os.path.join(output_dir, file_name)\n",
    "            filtered_data.to_netcdf(output_file_path)\n",
    "            dataset.close()\n",
    "            \n",
    "    print('Done spatially filtering files!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "60576843-1ff8-49f5-8695-4fe46dcc5af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NAM_combine_seq(input_dir, output_dir):\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    #Extract the date from the input directory name\n",
    "    dir_name = os.path.basename(input_dir)\n",
    "    date_str = dir_name.split('_')[1] \n",
    "    \n",
    "    nc_files = [os.path.join(input_dir, file) for file in os.listdir(input_dir) if file.endswith('.nc')]\n",
    "    nc_files.sort()\n",
    "    \n",
    "    datasets = [xr.open_dataset(nc_file) for nc_file in nc_files]\n",
    "    combined_dataset = xr.concat(datasets, dim='time')\n",
    "    \n",
    "\n",
    "    output_file_name = f'NAM_final_{date_str}.nc'\n",
    "    output_file_path = os.path.join(output_dir, output_file_name)\n",
    "    combined_dataset.to_netcdf(output_file_path, encoding={'time': {'units': 'hours since 2019-10-11'}})\n",
    "    \n",
    "    for ds in datasets:\n",
    "        ds.close()\n",
    "    \n",
    "    print(f'Combined dataset saved to {output_file_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d5e53b-e1e4-4a7b-ac73-af9554ba08cc",
   "metadata": {},
   "source": [
    "## Running functions to preprocess the NAM data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "76640faa-5a23-4d5b-bb16-8cf6eb996fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 1:\n",
    "\n",
    "input_dir = '/Users/gabbyvaillant/Downloads/BNL/0000_2019-10-11-files' \n",
    "output_dir = '/Users/gabbyvaillant/Downloads/BNL/0000_2019-10-11-files_filtered_test'\n",
    "variables = {'TMP_2maboveground': 'T2', 'UGRD_10maboveground': 'U10', 'VGRD_10maboveground': 'V10', 'PRES_surface': 'PSFC'}\n",
    "\n",
    "#Uncomment to run:\n",
    "#NAM_filter_vars(input_dir, output_dir, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "64224bae-05cd-4a9d-a990-8557d76d5225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done filtering files!\n"
     ]
    }
   ],
   "source": [
    "#STEP 2:\n",
    "\n",
    "input_dir = '/Users/gabbyvaillant/Downloads/BNL/0000_2019-10-11-files_filtered_test'\n",
    "output_dir = '/Users/gabbyvaillant/Downloads/BNL/0000_2019-10-11-files_fixed_dims'\n",
    "\n",
    "#Uncomment to run:\n",
    "#NAM_match_dims(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "a3591484-9273-4db8-a865-4d9f0a6b8a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done spatially filtering files!\n"
     ]
    }
   ],
   "source": [
    "#STEP 3:\n",
    "\n",
    "input_dir = '/Users/gabbyvaillant/Downloads/BNL/0000_2019-10-11-files_fixed_dims'\n",
    "output_dir = '/Users/gabbyvaillant/Downloads/BNL/0000_2019-10-11-stage1'\n",
    "\n",
    "#Uncomment to run:\n",
    "#NAM_spatial_filter(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "1df70926-f116-4a28-9161-581c3dcf287a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset saved to /Users/gabbyvaillant/Downloads/BNL/0000_2019-10-11-final-NAM/NAM_final_2019-10-11-stage1.nc\n"
     ]
    }
   ],
   "source": [
    "#STEP 4:\n",
    "\n",
    "input_dir = '/Users/gabbyvaillant/Downloads/BNL/0000_2019-10-11-stage1'\n",
    "output_dir = '/Users/gabbyvaillant/Downloads/BNL/0000_2019-10-11-final-NAM'\n",
    "\n",
    "#Uncomment to run:\n",
    "#NAM_combine_seq(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe258a0b-5678-45f7-9180-bcf4f5f775f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
