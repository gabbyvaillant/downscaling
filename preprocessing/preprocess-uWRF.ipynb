{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67ad08e0-d13e-4527-8232-5ee98a6e2bef",
   "metadata": {},
   "source": [
    "# Preprocessing uWRF Dataset\n",
    "\n",
    "Pre-req: Download uWRF output files from BNL's remote server (using command terminal).\n",
    "\n",
    "NOTE: using domain 02 (3km 3-hourly) becuase it has the same temporal aspect as NAM-NMM\n",
    "\n",
    "## Step 1:\n",
    "Use the uWRF_filter_vars function to filter the dataset down to the 4 variables of interest:\n",
    "\n",
    "* T2 (Temperature 2m above surface)\n",
    "* U10 (U component of wind 10m above surface)\n",
    "* V10 (V component of wind 10m above surface)\n",
    "* PSFC (Pressure at the surface)\n",
    "\n",
    "This function also change the names of 'XLAT', 'XLONG', 'XTIME' to 'latitude', 'longitude' and 'time'.\n",
    "\n",
    "## Step 2:\n",
    "\n",
    "Change the dimensions of the dataset from 'Time', 'north_south', 'west_east' to 'time', 'latitude', 'longitude.'\n",
    "\n",
    "## Step 3: \n",
    "Filter spatially to only include area covering Manhattan.\n",
    "\n",
    "min_lat = 40.57384924257281\n",
    "\n",
    "max_lat = 40.92\n",
    "\n",
    "min_lon = -74.0481110602903\n",
    "\n",
    "max_lon = -73.84627819243957\n",
    "\n",
    "## Step 4:\n",
    "Combine each day of data into a sequential format\n",
    "\n",
    "NOTE: \n",
    "\n",
    "Domain 2 (d02):\n",
    "\n",
    "time: 29 files up to 84 forecast hours (every 3 hours)\n",
    "\n",
    "spatial = 3 km "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "759de5fd-db3f-439d-badc-5959140fecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4\n",
    "import xarray as xarray\n",
    "import os\n",
    "import glob\n",
    "from netCDF4 import Dataset\n",
    "from scipy.interpolate import griddata\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "4dd174c7-bedf-4a2c-9d66-15e122303fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uWRF_filter_vars(input_dir, output_dir, variables):\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    #Only using the files for Domain 2\n",
    "    input_files = [file for file in glob.glob(os.path.join(input_dir, '*')) if 'd02' in os.path.basename(file)]\n",
    "    \n",
    "    for file in input_files:\n",
    "        \n",
    "        ds = xr.open_dataset(file)\n",
    "        \n",
    "        #Filter out the variables\n",
    "        ds_filtered = ds[variables]\n",
    "\n",
    "        #Change XLAT, XLON, XTIME\n",
    "        ds_filtered = ds_filtered.rename({'XLAT': 'latitude'})\n",
    "        ds_filtered = ds_filtered.rename({'XLONG': 'longitude'})\n",
    "        ds_filtered = ds_filtered.rename({'XTIME': 'time'})\n",
    "\n",
    "        #Saving and closing files\n",
    "        filename = os.path.basename(file)\n",
    "        output_file = os.path.join(output_dir, filename)\n",
    "        ds_filtered.to_netcdf(output_file)\n",
    "        ds.close()\n",
    "        ds_filtered.close()\n",
    "\n",
    "    print('Done filtering files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "d06dfff0-152d-4cb4-8a18-46d13ffa518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uWRF_match_dims(input_dir, output_dir):\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    input_files = glob.glob(os.path.join(input_dir, '*'))  # Handle all uWRF files\n",
    "    \n",
    "    for file_name in input_files:\n",
    "        \n",
    "        ds = xr.open_dataset(file_name)\n",
    "\n",
    "        lat_values = ds['latitude'].values \n",
    "        lon_values = ds['longitude'].values \n",
    "        time = ds['time']\n",
    "        \n",
    "        latitudes = lat_values[0,:,:] #take all the lat values from the first time step\n",
    "        longitudes = lon_values[0,:,:] #take all of the lon values from the first time step\n",
    "        \n",
    "        lat_attrs = ds['latitude'].attrs\n",
    "        lon_attrs = ds['longitude'].attrs\n",
    "        time_attrs = ds['time'].attrs\n",
    "        \n",
    "        #Flatten latitude and longitude for interpolation\n",
    "        #zip gets pairs and flatten makes them 1d arrays\n",
    "        points = np.array([(lon, lat) for lat, lon in zip(latitudes.flatten(), longitudes.flatten())])\n",
    "        \n",
    "        #Define the new latitude and longitude grid\n",
    "        new_latitudes = np.linspace(np.min(latitudes), np.max(latitudes), num=latitudes.shape[0])\n",
    "        new_longitudes = np.linspace(np.min(longitudes), np.max(longitudes), num=longitudes.shape[1])\n",
    "        \n",
    "        new_lon_grid, new_lat_grid = np.meshgrid(new_longitudes, new_latitudes)\n",
    "        \n",
    "        new_vars = {}\n",
    "        \n",
    "        for var_name in ds.data_vars:\n",
    "            var = ds[var_name]\n",
    "            new_var_list = []\n",
    "            \n",
    "            for t in range(var.sizes['Time']):\n",
    "                weather_variable = var.isel(Time=t).values  # Extract the data for the time step\n",
    "                \n",
    "                #Flatten the weather variable data\n",
    "                values = weather_variable.flatten()\n",
    "                \n",
    "                #Interpolate the data onto the new grid\n",
    "                new_weather_variable = griddata(points, values, (new_lon_grid, new_lat_grid), method='linear')\n",
    "                \n",
    "                #Append the interpolated data for the current time step\n",
    "                new_var_list.append(new_weather_variable)\n",
    "            \n",
    "            new_vars[var_name] = (['time', 'latitude', 'longitude'], np.stack(new_var_list))\n",
    "        \n",
    "        new_ds = xr.Dataset(\n",
    "            new_vars, coords={'latitude': new_latitudes,\n",
    "                              'longitude': new_longitudes,\n",
    "                              'time': ds['time'].values})\n",
    "        \n",
    "        new_ds['latitude'].attrs.update(lat_attrs)\n",
    "        new_ds['longitude'].attrs.update(lon_attrs)\n",
    "        new_ds['time'].attrs.update(time_attrs)\n",
    "        \n",
    "        for var_name in ds.data_vars:\n",
    "            new_ds[var_name].attrs.update(ds[var_name].attrs)\n",
    "            \n",
    "        #Drop the 'Time' dimension\n",
    "        if 'Time' in new_ds.dims:\n",
    "            new_ds = new_ds.drop_dims('Time')\n",
    "\n",
    "        new_ds.attrs.update(ds.attrs)\n",
    "        \n",
    "        output_file_name = os.path.basename(file_name)\n",
    "        output_file_path = os.path.join(output_dir, output_file_name)\n",
    "        new_ds.to_netcdf(output_file_path)\n",
    "        \n",
    "        ds.close()\n",
    "        \n",
    "    print('Done regridding uWRF files!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "6a5813dc-f178-4de4-b0e2-fb9f9a4891e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uWRF_spatial_filtering(input_dir, output_dir):\n",
    "\n",
    "    #Bounds to cover Manhattan\n",
    "    min_lat = 40.57384924257281\n",
    "    max_lat = 40.92\n",
    "    min_lon = -74.0481110602903\n",
    "    max_lon = -73.84627819243957\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok = True)\n",
    "    input_files = glob.glob(os.path.join(input_dir, '*')) #don't specify .nc bc for some reason uWRF aren't .nc files\n",
    "    \n",
    "    for file in input_files:\n",
    "        ds = xr.open_dataset(file)\n",
    "\n",
    "        #Extract latitude and longitude variables\n",
    "        lat_var = 'latitude'\n",
    "        lon_var = 'longitude'\n",
    "        lat = ds[lat_var].values\n",
    "        lon = ds[lon_var].values\n",
    "\n",
    "        #Filter the data based off of the spatial bounds\n",
    "        filtered_data = ds.where(\n",
    "            (ds[lat_var] >= min_lat) & (ds[lat_var] <= max_lat) &\n",
    "            (ds[lon_var] >= min_lon) & (ds[lon_var] <= max_lon), drop=True)\n",
    "\n",
    "        \n",
    "        filename = os.path.basename(file)\n",
    "        output_file_path = os.path.join(output_dir, filename)\n",
    "        filtered_data.to_netcdf(output_file_path)\n",
    "        ds.close()\n",
    "        filtered_data.close()\n",
    "        \n",
    "    print('Done spatially filtering files!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "a5826bc6-2af4-401e-be22-118046766a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uWRF_combine_seq(input_dir, output_dir):\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    dir_name = os.path.basename(input_dir)\n",
    "    date_str = dir_name.split('-')[1]\n",
    "    \n",
    "    input_files = glob.glob(os.path.join(input_dir, '*'))  # Don't specify .nc because uWRF files aren't .nc files\n",
    "    input_files.sort()\n",
    "\n",
    "    datasets = [xr.open_dataset(file) for file in input_files]\n",
    "\n",
    "    #Concatenate datasets along the shared 'time' dimension\n",
    "    combined_dataset = xr.concat(datasets, dim='time')\n",
    "\n",
    "    output_file_name = f'uWRF_final_{date_str}.nc'\n",
    "    output_file_path = os.path.join(output_dir, output_file_name)\n",
    "    \n",
    "    combined_dataset.to_netcdf(output_file_path, encoding={'time': {'units': 'hours since 2019-10-11'}}) #idk if this works\n",
    "    \n",
    "    combined_dataset.close()\n",
    "    for ds in datasets:\n",
    "        ds.close()\n",
    "    \n",
    "    print(f'Combined dataset saved to {output_file_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc22b800-6569-44f6-ac97-40ec3743a837",
   "metadata": {},
   "source": [
    "## Running functions to preprocess the uWRF data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "847cf5a9-007d-4040-981e-d073ca29cd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done filtering files\n"
     ]
    }
   ],
   "source": [
    "#STEP 1:\n",
    "input_dir = '/Users/gabbyvaillant/Downloads/BNL/og_uWRF_files/NYC_wrfout_20191011' \n",
    "output_dir = '/Users/gabbyvaillant/Downloads/BNL/uWRF-20191011-filtered'\n",
    "variables = ['T2', 'U10', 'V10', 'PSFC']\n",
    "\n",
    "#Uncomment to do filtering\n",
    "#uWRF_filter_vars(input_dir, output_dir, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "8345f787-491e-4b36-9b86-944dd9cb9eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done regridding uWRF files!\n"
     ]
    }
   ],
   "source": [
    "#STEP 2:\n",
    "input_dir = '/Users/gabbyvaillant/Downloads/BNL/uWRF-20191011-filtered'\n",
    "output_dir = '/Users/gabbyvaillant/Downloads/BNL/uWRF-20191011-fixed_dims'\n",
    "\n",
    "#uWRF_match_dims(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "154f6966-8f84-4c1c-93c9-92ec19653735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done spatially filtering files!\n"
     ]
    }
   ],
   "source": [
    "#STEP 3:\n",
    "input_dir = '/Users/gabbyvaillant/Downloads/BNL/uWRF-20191011-fixed_dims'\n",
    "output_dir = '/Users/gabbyvaillant/Downloads/BNL/uWRF-20191011-spatial'\n",
    "\n",
    "#Uncomment to do filtering\n",
    "#uWRF_spatial_filtering(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "61fe15fb-67b7-4e50-b23f-89dba714dc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset saved to /Users/gabbyvaillant/Downloads/BNL/final-uWRF-files/uWRF_final_20191011.nc\n"
     ]
    }
   ],
   "source": [
    "#STEP 4:\n",
    "input_dir = '/Users/gabbyvaillant/Downloads/BNL/uWRF-20191011-spatial'\n",
    "output_dir = '/Users/gabbyvaillant/Downloads/BNL/final-uWRF-files'\n",
    "\n",
    "#Uncomment to do filtering\n",
    "#uWRF_combine_seq(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025efd16-dba3-47ce-9bcf-b332ee4e4de6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
