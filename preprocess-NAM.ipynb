{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0121312-b916-42c2-8a5b-aaa5c649dfd9",
   "metadata": {},
   "source": [
    "# Preprocessing NAM-NMM Dataset\n",
    "\n",
    "## Step 1:\n",
    "\n",
    "Retrieve NAM-NMM dataset from BNL's remote servers using scp command.\n",
    "\n",
    "## Step 2:\n",
    "Use the filter_vars function to filter the dataset down to the variables of interest: \n",
    "* TMP_2maboveground\n",
    "* UGRD_10maboveground\n",
    "* VGRD_10maboveground\n",
    "* PRES_surface\n",
    "\n",
    "## Step 3: \n",
    "Filter spatially to only include area covering Manhattan\n",
    "\n",
    "## Step 4:\n",
    "Combine each day of data into a sequential format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35889eb1-f3c8-48ac-a452-1a6d5813aaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4\n",
    "import xarray as xarray\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54d22bd3-84dc-47e6-be6c-075deccd13cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 2:\n",
    "def filter_vars(input_dir, output_dir, variables):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Filter netCDF files down to contain variables of interest\n",
    "    \n",
    "    Args:\n",
    "    input_dir: directory on computer holding orignal netCDF files\n",
    "    output_dir: directory on computer where you want the filtered datasets to be stored\n",
    "    variables: list of variables to keep after filtering\n",
    "\n",
    "    Returns:\n",
    "    \n",
    "    None. Filtered datasets placed in the specified output_dir.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    #Get all the netCDF files in directory\n",
    "    input_files = glob.glob(os.path.join(input_dir, '*.nc'))\n",
    "    \n",
    "    #Loop through all the files in the input_dir\n",
    "    for file in input_files:\n",
    "        #Read and open the file\n",
    "        data = xr.open_dataset(file)\n",
    "\n",
    "        #Only keep the selected variables\n",
    "        data_filtered = data[variables]\n",
    "        \n",
    "        #Create the output file path\n",
    "        filename = os.path.basename(file)\n",
    "        output_file = os.path.join(output_dir, filename)\n",
    "\n",
    "        #Save the file to a new NetCDF file\n",
    "        data_filtered.to_netcdf(output_file)\n",
    "        data.close()\n",
    "        data_filtered.close()\n",
    "\n",
    "\n",
    "    print('Done filtering files!')\n",
    "\n",
    "\n",
    "#Args to get data for October 11th 2019\n",
    "input_dir = '/Users/gabbyvaillant/Downloads/BNL/0000_2019-10-11-files' \n",
    "output_dir = '/Users/gabbyvaillant/Downloads/BNL/0000_2019-10-11-files_filtered'\n",
    "variables = ['TMP_2maboveground', 'UGRD_10maboveground', 'VGRD_10maboveground', 'PRES_surface']\n",
    "\n",
    "#Uncomment to do filtering\n",
    "#filter_vars(input_dir, output_dir, variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2fd17f7-602c-4473-b80a-4577f5b063fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 3:\n",
    "def stage_1_filtering(file_path, output_dir):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Spatially filtering the datasets to only inlcude data representing Manhattan (Stage 1)\n",
    "\n",
    "    Args:\n",
    "    file_path: path to file that you want to spatially filter.\n",
    "    output_directory: path to directory where you wanted the spatially filtered dataset to be stored.\n",
    "\n",
    "    Returns:\n",
    "    None. Spatially filtered netCDF files is placed in the specified output_dir.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset = xr.open_dataset(file_path)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok = True)\n",
    "    \n",
    "    #These x and y values are used to define the bounds of Stage 1\n",
    "    x_min = 572977.0\n",
    "    x_max = 585168.0\n",
    "    y_min = 207247.0\n",
    "    y_max = 243820.0\n",
    "    \n",
    "    #Extract the necessary variables\n",
    "    x = dataset['x']\n",
    "    y = dataset['y']\n",
    "    \n",
    "    #Filter the data based on the `(y, x)` bounds\n",
    "    filtered_data = dataset.where(\n",
    "        (x >= x_min) & (x <= x_max) &\n",
    "        (y >= y_min) & (y <= y_max), drop=True\n",
    "    )\n",
    "\n",
    "    #Construct the output file path\n",
    "    output_file_path = os.path.join(output_dir, os.path.basename(file_path))\n",
    "    \n",
    "    #Save the filtered data to a new NetCDF file\n",
    "    filtered_data.to_netcdf(output_file_path)\n",
    "    \n",
    "    dataset.close()\n",
    "\n",
    "\n",
    "#Directory on my computer holding all the original oct 11 2019 files:\n",
    "input_dir = '/Users/gabbyvaillant/Downloads/BNL/0000_2019-10-11-files_filtered'\n",
    "\n",
    "#Where I want the filtered files to go:\n",
    "output_dir = '/Users/gabbyvaillant/Downloads/BNL/stage-1-files'\n",
    "\n",
    "#Uncomment to create the files: \n",
    "\n",
    "\"\"\"\n",
    "#Loop through all files in the input directory and apply the filtering function\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.nc'):  # Ensure you are processing only NetCDF files\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "        stage_1_filtering(file_path, output_dir)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcca065e-edc1-4f54-aee8-38762771452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 4:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Taking all the files from stage 1 for 10/11/2019 and combining them into one file in sequential order.\n",
    "Dimension for stage 1: (time: 29, y: 4, x: 2)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#Directory containing the stage 1 files:\n",
    "stage1_file_dir = '/Users/gabbyvaillant/Downloads/BNL/stage-1-files'\n",
    "output_file_path = '/Users/gabbyvaillant/Downloads/BNL/stage1-sequential/stage1_20191011_seq.nc'\n",
    "'/Users/gabbyvaillant/Downloads/BNL/final-files/NAM/NAM_2019{}{}_final.nc'\n",
    "\n",
    "\n",
    "nc_files = [os.path.join(stage1_file_dir, file) for file in os.listdir(stage1_file_dir) if file.endswith('.nc')]\n",
    "\n",
    "#Sort files in sequential order\n",
    "#this may be what is getting rid of the forecast hour after 24\n",
    "nc_files.sort()\n",
    "\n",
    "# Open all files as xarray datasets and combine them along the time dimension\n",
    "datasets = [xr.open_dataset(nc_file) for nc_file in nc_files]\n",
    "\n",
    "#Merge on time dimension\n",
    "combined_dataset = xr.concat(datasets, dim='time')\n",
    "combined_dataset.to_netcdf(output_file_path)\n",
    "\n",
    "for ds in datasets:\n",
    "    ds.close()\n",
    "\n",
    "print(f'Combined dataset saved to {output_file_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
