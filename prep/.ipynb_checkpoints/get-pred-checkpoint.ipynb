{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70ec0335-8020-432c-9b2b-68fbf3a57862",
   "metadata": {},
   "source": [
    "# Preprocessing NAM and uWRF with Predictor Variable\n",
    "\n",
    "The functions in this notebook are used to preprocess both datasets to be ready to be inputted into the deep learning model. The first half of this notebook focuses on processing our high resolution uWRF data and the rest preprocesses the low resolution NAM data.\n",
    "\n",
    "Cell 0: Importing necessary libraries.\n",
    "\n",
    "Cell 1: uWRF_filter_vars_with_pred\n",
    "Renames important variable to standard name, and only keeps the downscaling variable of interest and the associated predictor variable. These variables are chosen by the user. \n",
    "\n",
    "Cell 2: uWRF_match_dims_with_pred\n",
    "Assigns new dimensions to the dataset: time, latitude, longitude. Perserves the global and variable attributes\n",
    "\n",
    "Cell 3: uWRF_combine_seq\n",
    "Combines each dataset in sequtial order along the time dimension to form the training set.\n",
    "\n",
    "Cell 4: uWRF_val_test\n",
    "Combines each 3-hourly dataset in sequential order along the time dimension to form either the validation set or test set. The user must specify which dataset they want to create with this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e333358-ba69-4273-81af-dfc09692eb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from scipy.interpolate import griddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c10a9aa3-e096-4b4c-adc1-cf767138eb83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def uWRF_filter_vars_with_pred(input_dir, output_dir, variables):\n",
    "\n",
    "    input_files = glob.glob(os.path.join(input_dir, '*'))\n",
    "    \n",
    "    for file in input_files:\n",
    "        \n",
    "        print(f\"Processing file: {file}\")\n",
    "        ds = xr.open_dataset(file)\n",
    "        \n",
    "        #Keep variable choosen by the user. This should be the variable being downscaled and an associated predictor variable\n",
    "        ds_filtered = ds[variables]\n",
    "\n",
    "        #Rename variables\n",
    "        ds_filtered = ds_filtered.rename({'XLAT': 'latitude', 'XLONG': 'longitude', 'XTIME': 'time'})\n",
    "\n",
    "        filename = os.path.basename(file)\n",
    "        output_file = os.path.join(output_dir, filename)\n",
    "        print(f\"Saving file to: {output_file}\")\n",
    "        ds_filtered.to_netcdf(output_file)\n",
    "       \n",
    "def main():\n",
    "    \n",
    "    for i in range(1, 32):\n",
    "        remote_input_dir = f\"/D4/data/gvaillant/uwrf/03/{str(i).zfill(2)}/d02_files\" #Use either d02 files or d03 files\n",
    "        print(f\"Processing directory: {remote_input_dir}\")\n",
    "        \n",
    "        remote_output_dir = f\"/D4/data/gvaillant/prep-uwrf/d02/pred-stage1/03/{str(i).zfill(2)}\"\n",
    "        print(f\"Output directory: {remote_output_dir}\")\n",
    "        \n",
    "        uWRF_filter_vars_with_pred(remote_input_dir, remote_output_dir, ['T2', 'PSFC']) #User chooses the variables \n",
    "            \n",
    "    print(\"Done processing stage1 uWRF files with predictor!\")\n",
    "\n",
    "#Uncomment below to run:\n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c08b9dd1-35f4-4f0e-8f65-9ea8580bb313",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def uWRF_match_dims_with_pred(input_dir, output_dir):\n",
    "    \n",
    "    input_files = glob.glob(os.path.join(input_dir, '*'))\n",
    "\n",
    "    for file_name in input_files:\n",
    "        \n",
    "        print(f\"Processing file: {file_name}\")\n",
    "        ds = xr.open_dataset(file_name)\n",
    "\n",
    "        #Extract latitude, longitude, and time values (these will be the dimensions of the dataset)\n",
    "        lat_values = ds['latitude'].values[0, :, :]  # Use the first timestep for latitudes *CHECKKKKKKK*\n",
    "        lon_values = ds['longitude'].values[0, :, :]  # Use the first timestep for longitudes\n",
    "        time = ds['time']\n",
    "\n",
    "        #Preserve attributes\n",
    "        lat_attrs = ds['latitude'].attrs\n",
    "        lon_attrs = ds['longitude'].attrs\n",
    "        time_attrs = ds['time'].attrs\n",
    "\n",
    "        #Reorganize dataset dimensions\n",
    "        new_vars = {}\n",
    "        for var_name in ds.data_vars:\n",
    "            var = ds[var_name]\n",
    "            new_vars[var_name] = (['time', 'latitude', 'longitude'], var.values)  #Reassign dimensions\n",
    "\n",
    "        #Create a new dataset with updated dimensions\n",
    "        new_ds = xr.Dataset(\n",
    "            new_vars,\n",
    "            coords={\n",
    "                'latitude': (['latitude'], lat_values[:, 0]),  # Convert to 1D\n",
    "                'longitude': (['longitude'], lon_values[0, :]),  # Convert to 1D\n",
    "                'time': time.values\n",
    "            }\n",
    "        )\n",
    "\n",
    "        #Add attributes\n",
    "        new_ds['latitude'].attrs.update(lat_attrs)\n",
    "        new_ds['longitude'].attrs.update(lon_attrs)\n",
    "        new_ds['time'].attrs.update(time_attrs)\n",
    "\n",
    "        #Add variable attributes\n",
    "        for var_name in ds.data_vars:\n",
    "            new_ds[var_name].attrs.update(ds[var_name].attrs)\n",
    "\n",
    "        #Add global attributes\n",
    "        new_ds.attrs.update(ds.attrs)\n",
    "\n",
    "        output_file_name = os.path.basename(file_name)\n",
    "        output_file_path = os.path.join(output_dir, output_file_name)\n",
    "        new_ds.to_netcdf(output_file_path)\n",
    "        print(f\"Saved file to: {output_file_path}\")\n",
    "\n",
    "    print(\"Done with uWRF dimension adjustment!\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    for i in range(1, 31):\n",
    "        input_dir = f\"/D4/data/gvaillant/prep-uwrf/d02/pred-stage1/03/{str(i).zfill(2)}\"\n",
    "        print(f\"Processing directory: {input_dir}\")\n",
    "\n",
    "        output_dir = f\"/D4/data/gvaillant/prep-uwrf/d02/pred-stage2/03/{str(i).zfill(2)}\"\n",
    "        print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "        uWRF_match_dims_with_pred(input_dir, output_dir)\n",
    "        print(f\"Done processing directory: {input_dir}\")\n",
    "\n",
    "    print(\"Done processing stage2 uWRF files!\")\n",
    "\n",
    "#Uncomment below to run:\n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985003e2-9290-4b2f-a323-3a45526f8a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this\n",
    "\n",
    "def adjust_bounds_to_match(lat_values, lon_values, min_lat, max_lat, min_lon, max_lon):\n",
    "    \"\"\"\n",
    "    Adjust bounds until the number of latitude and longitude values is the same,\n",
    "    neither is a prime number, and both are divisible by 4.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        lat_count = len(lat_values[(lat_values >= min_lat) & (lat_values <= max_lat)])\n",
    "        lon_count = len(lon_values[(lon_values >= min_lon) & (lon_values <= max_lon)])\n",
    "\n",
    "        # Check if BOTH conditions are met\n",
    "        if lat_count == lon_count and lat_count % 4 == 0:\n",
    "            break  # Conditions satisfied: same count, not prime, divisible by 4\n",
    "\n",
    "        # Adjust bounds to change the count\n",
    "        if lat_count != lon_count:\n",
    "            if lat_count < lon_count:\n",
    "                max_lat += (lat_values[1] - lat_values[0])  # Increment latitude bound\n",
    "            else:\n",
    "                max_lon += (lon_values[1] - lon_values[0])  # Increment longitude bound\n",
    "        else:  # If counts are equal but not meeting the conditions\n",
    "            max_lat += (lat_values[1] - lat_values[0])  # Increment latitude bound\n",
    "            max_lon += (lon_values[1] - lon_values[0])  # Increment longitude bound\n",
    "\n",
    "    return min_lat, max_lat, min_lon, max_lon\n",
    "\n",
    "\n",
    "def uWRF_spatial_cut(input_dir, output_dir, min_lat, max_lat, min_lon, max_lon):\n",
    "    \"\"\"\n",
    "    Function to spatially filter uWRF data to cover NYC, the boroughs, and ensure \n",
    "    consistent latitude and longitude dimensions, avoiding prime numbers and ensuring divisibility by 4.\n",
    "    \"\"\"\n",
    "    input_files = glob.glob(os.path.join(input_dir, '*'))\n",
    "\n",
    "    for file in input_files:\n",
    "        print(f\"Processing file: {file}\")\n",
    "        ds = xr.open_dataset(file)\n",
    "\n",
    "        lat = ds['latitude']\n",
    "        lon = ds['longitude']\n",
    "\n",
    "        # Handle 1D or 2D coordinates dynamically\n",
    "        if lat.ndim == 1 and lon.ndim == 1:\n",
    "            # 1D Coordinates\n",
    "            lat_values = lat.values\n",
    "            lon_values = lon.values\n",
    "\n",
    "            # Adjust bounds to match dimensions and avoid prime numbers\n",
    "            min_lat, max_lat, min_lon, max_lon = adjust_bounds_to_match(\n",
    "                lat_values, lon_values, min_lat, max_lat, min_lon, max_lon\n",
    "            )\n",
    "\n",
    "            filtered_data = ds.sel(\n",
    "                latitude=slice(min_lat, max_lat),\n",
    "                longitude=slice(min_lon, max_lon)\n",
    "            )\n",
    "        else:\n",
    "            # 2D Coordinates\n",
    "            lat_mask = (lat >= min_lat) & (lat <= max_lat)\n",
    "            lon_mask = (lon >= min_lon) & (lon <= max_lon)\n",
    "            combined_mask = lat_mask & lon_mask\n",
    "\n",
    "            filtered_data = ds.where(combined_mask, drop=True)\n",
    "\n",
    "            # Ensure lat/lon counts are equal, not prime, and divisible by 4\n",
    "            lat_values = filtered_data['latitude'].values\n",
    "            lon_values = filtered_data['longitude'].values\n",
    "            min_lat, max_lat, min_lon, max_lon = adjust_bounds_to_match(\n",
    "                lat_values, lon_values, min_lat, max_lat, min_lon, max_lon\n",
    "            )\n",
    "\n",
    "        # Save the output\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        filename = os.path.basename(file)\n",
    "        output_file = os.path.join(output_dir, filename)\n",
    "        print(f\"Saving file to: {output_file}\")\n",
    "        filtered_data.to_netcdf(output_file)\n",
    "\n",
    "\n",
    "def main():\n",
    "    for i in range(1, 29):\n",
    "        input_dir = f\"/D4/data/gvaillant/prep-uwrf/d02/pred-stage2/03/{str(i).zfill(2)}\"\n",
    "        print(f\"Processing directory: {input_dir}\")\n",
    "\n",
    "        output_dir = f\"/D4/data/gvaillant/prep-uwrf/d02/pred-stage3/03/{str(i).zfill(2)}\"\n",
    "        print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "        # Bounds to cover NYC and Boroughs\n",
    "        min_lat = 40.4774\n",
    "        max_lat = 40.9176\n",
    "        min_lon = -74.2591\n",
    "        max_lon = -73.7004\n",
    "\n",
    "        uWRF_spatial_cut(input_dir, output_dir, min_lat, max_lat, min_lon, max_lon)\n",
    "        print(f\"Done spatially filtering the uWRF files to NYC!\")\n",
    "\n",
    "\n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e844c51-2f7f-4149-999e-9a50783a8340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import xarray as xr\n",
    "\n",
    "def uWRF_combine_seq(input_dirs, output_dir):\n",
    "    combined_datasets = []\n",
    "    \n",
    "    for input_dir in input_dirs:\n",
    "        # Loop through each day's subdirectory within the monthly directory\n",
    "        day_dirs = sorted(next(os.walk(input_dir))[1])  # Get list of day subdirectories\n",
    "        for day_dir in day_dirs:\n",
    "            day_path = os.path.join(input_dir, day_dir)  # Full path to each day's subdirectory\n",
    "            input_files = sorted(glob.glob(os.path.join(day_path, '*')))  # Gather all files in the daily subdirectory\n",
    "            \n",
    "            # Load each file into a dataset and add to the list\n",
    "            datasets = [xr.open_dataset(file) for file in input_files]\n",
    "            combined_datasets.extend(datasets)\n",
    "\n",
    "    # Concatenate all datasets along the 'time' dimension\n",
    "    combined_dataset = xr.concat(combined_datasets, dim='time')\n",
    "\n",
    "    # Generate output filename based on month range from input directories\n",
    "    month_range = \"-\".join([os.path.basename(month_dir) for month_dir in input_dirs])\n",
    "    output_file_name = f'uWRF_final_{month_range}.nc'\n",
    "    output_file_path = os.path.join(output_dir, output_file_name)\n",
    "\n",
    "    # Save the concatenated dataset to a NetCDF file\n",
    "    combined_dataset.to_netcdf(output_file_path, encoding={'time': {'units': 'hours since 2019-10-11'}}) #that isnt correct\n",
    "    print(f'Combined dataset saved to {output_file_path}')\n",
    "\n",
    "def main():\n",
    "    input_dirs = [\n",
    "        \"/D4/data/gvaillant/prep-uwrf/d02/pred-stage3/01\",\n",
    "        \"/D4/data/gvaillant/prep-uwrf/d02/pred-stage3/02\"\n",
    "    ]\n",
    "    output_dir = '/D4/data/gvaillant/prep-uwrf/d02/pred-NYC-split/train'\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "    uWRF_combine_seq(input_dirs, output_dir)\n",
    "    print(\"Done combining uWRF files!\")\n",
    "\n",
    "#Uncomment below to run:\n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e7f43bb-b344-483b-9df6-a56b4466b022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uWRF_val_test(input_dirs, output_dir):\n",
    "    combined_datasets = []\n",
    "    \n",
    "    for input_dir in input_dirs:\n",
    "        # Loop through each day's subdirectory within the monthly directory\n",
    "        day_dirs = sorted(next(os.walk(input_dir))[1])  # Get list of day subdirectories\n",
    "        for day_dir in day_dirs:\n",
    "            day_path = os.path.join(input_dir, day_dir)  # Full path to each day's subdirectory\n",
    "            nc_files = sorted(glob.glob(os.path.join(day_path, '*')))  # Gather all files in the daily subdirectory\n",
    "\n",
    "            # Take the first half of the files\n",
    "            half_length = len(nc_files) // 2\n",
    "            #selected_files = nc_files[:half_length]  # Select first half of the sorted files (VALIDATION)\n",
    "            selected_files = nc_files[half_length:] #Select second half of the sorted files (TESTING)\n",
    "            \n",
    "            # Open the selected files\n",
    "            # Load each file into a dataset and add to the list\n",
    "            datasets = [xr.open_dataset(file) for file in selected_files]\n",
    "            combined_datasets.extend(datasets)\n",
    "\n",
    "    # Concatenate all datasets along the 'time' dimension\n",
    "    combined_dataset = xr.concat(combined_datasets, dim='time')\n",
    "\n",
    "    # Generate output filename based on month range from input directories\n",
    "    month_range = \"-\".join([os.path.basename(month_dir) for month_dir in input_dirs])\n",
    "    output_file_name = f'uWRF_final_{month_range}.nc'\n",
    "    output_file_path = os.path.join(output_dir, output_file_name)\n",
    "\n",
    "    # Save the concatenated dataset to a NetCDF file\n",
    "    combined_dataset.to_netcdf(output_file_path, encoding={'time': {'units': 'hours since 2019-10-11'}})\n",
    "    print(f'Combined dataset saved to {output_file_path}')\n",
    "\n",
    "def main():\n",
    "    input_dirs = [\n",
    "        \"/D4/data/gvaillant/prep-uwrf/d02/pred-stage3/03\"\n",
    "    ]\n",
    "    output_dir = '/D4/data/gvaillant/prep-uwrf/d02/pred-NYC-split/test' #change to val or test\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "    uWRF_combine_seq(input_dirs, output_dir)\n",
    "    print(\"Done combining uWRF files!\")\n",
    "\n",
    "#Uncomment below to run:\n",
    "#main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8a3e97-dc9e-447a-bf09-db960bc42d26",
   "metadata": {},
   "source": [
    "# NAM functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2ec26baa-fe4c-4942-adab-0c595d881641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import netCDF4\n",
    "import xarray as xr\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "def NAM_filter_and_match_dims(input_dir, output_dir, variables):\n",
    "    # List all files in input directory\n",
    "    file_list = [(os.path.join(input_dir, file), file) for file in os.listdir(input_dir) if file.endswith('.nc')]\n",
    "\n",
    "    for file_path, file_name in file_list:  # Unpack the tuple\n",
    "        try:\n",
    "            # Step 1: Filter the variables using NAM_filter_vars logic\n",
    "            with xr.open_dataset(file_path) as ds:\n",
    "                existing_vars = {var: ds[var] for var in variables.keys() if var in ds}\n",
    "                if not existing_vars:\n",
    "                    print(f\"No matching variables found in {file_name}.\")\n",
    "                    continue\n",
    "\n",
    "                # Filter and rename variables\n",
    "                ds_filtered = xr.Dataset(existing_vars).rename(variables)\n",
    "\n",
    "                for var in ds_filtered.data_vars:\n",
    "                    if 'time' in ds_filtered[var].dims:\n",
    "                        dims = ('time',) + tuple(d for d in ds_filtered[var].dims if d != 'time')\n",
    "                        ds_filtered[var] = ds_filtered[var].transpose(*dims)\n",
    "\n",
    "                for orig_var, new_var in variables.items():\n",
    "                    if orig_var in ds:\n",
    "                        ds_filtered[new_var].attrs = ds[orig_var].attrs\n",
    "\n",
    "                ds_filtered.attrs = ds.attrs\n",
    "\n",
    "                # Change longitude values to be in degrees west\n",
    "                if 'longitude' in ds_filtered:\n",
    "                    lon = ds_filtered['longitude'].values\n",
    "                    lon = np.where(lon > 180, lon - 360, lon)\n",
    "                    ds_filtered['longitude'].values = lon\n",
    "                    ds_filtered['longitude'].attrs['units'] = 'degrees_west'\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_name}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Step 2: Interpolate variables using NAM_match_dims logic\n",
    "        latitudes = ds_filtered['latitude'].values  # Shape: (67, 71)\n",
    "        longitudes = ds_filtered['longitude'].values  # Shape: (67, 71)\n",
    "        time = ds_filtered['time']\n",
    "        \n",
    "        # Save all the attributes for each variable\n",
    "        lat_attrs = ds_filtered['latitude'].attrs\n",
    "        lon_attrs = ds_filtered['longitude'].attrs\n",
    "        time_attrs = ds_filtered['time'].attrs\n",
    "        \n",
    "        # Flatten latitude and longitude for interpolation\n",
    "        points = np.array([(lon, lat) for lat_row, lon_row in zip(latitudes, longitudes) for lat, lon in zip(lat_row, lon_row)])\n",
    "        \n",
    "        # Define the new latitude and longitude grid\n",
    "        new_latitudes = np.linspace(np.min(latitudes), np.max(latitudes), num=67)\n",
    "        new_longitudes = np.linspace(np.min(longitudes), np.max(longitudes), num=67)\n",
    "        \n",
    "        # Create new meshgrid\n",
    "        new_lon_grid, new_lat_grid = np.meshgrid(new_longitudes, new_latitudes)\n",
    "        \n",
    "        new_vars = {}\n",
    "        \n",
    "        for var_name in ds_filtered.data_vars:\n",
    "            var = ds_filtered[var_name]\n",
    "            new_var_list = []\n",
    "            \n",
    "            for t in range(len(var.time)):\n",
    "                weather_variable = var.values[t, :, :]  # Shape (67, 71)\n",
    "                \n",
    "                # Flatten the weather variable data\n",
    "                values = weather_variable.flatten()\n",
    "                \n",
    "                # Interpolate the data onto the new grid\n",
    "                new_weather_variable = griddata(points, values, (new_lon_grid, new_lat_grid), method='linear')\n",
    "                \n",
    "                # Append the interpolated data for the current time step\n",
    "                new_var_list.append(new_weather_variable)\n",
    "            \n",
    "            # Stack the new variables along the time dimension\n",
    "            new_vars[var_name] = (['time', 'latitude', 'longitude'], np.stack(new_var_list))\n",
    "        \n",
    "        # Create a new xarray Dataset\n",
    "        new_ds = xr.Dataset(\n",
    "            new_vars, coords={'latitude': new_latitudes,\n",
    "                              'longitude': new_longitudes,\n",
    "                              'time': time.values})\n",
    "        \n",
    "        # Add the original variable attributes\n",
    "        new_ds['time'].attrs.update(time_attrs)\n",
    "        new_ds['latitude'].attrs.update(lat_attrs)\n",
    "        new_ds['longitude'].attrs.update(lon_attrs)\n",
    "\n",
    "        # Add global attributes\n",
    "        new_ds.attrs.update(ds_filtered.attrs)\n",
    "\n",
    "        #Saving files\n",
    "        filename = os.path.basename(file_name)\n",
    "        output_file = os.path.join(output_dir, filename)\n",
    "        print(f\"Saving file to: {output_file}\")  # Print the output file path\n",
    "        new_ds.to_netcdf(output_file)\n",
    "\n",
    "def main():\n",
    "    input_dir = \"/D4/data/gvaillant/NAM-2019-netcdf/03\"\n",
    "    output_dir = \"/D4/data/gvaillant/NAM/2019/match-pred/03\"\n",
    "    variables = {'TMP_2maboveground': 'T2', 'PRES_surface': 'PRES'}\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    \n",
    "    NAM_filter_and_match_dims(input_dir, output_dir, variables)\n",
    "    \n",
    "    print(\"Done processing stage1 NAM files!\")\n",
    "\n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e5fceaf-0fe1-4f86-87c7-133f376b5afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import os\n",
    "#need to get it cut down to around the same area as uWRF\n",
    "\n",
    "def NAM_spatial_filter(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Function to load the NAM dataset and spatially cut it to the uWRF bounds.\n",
    "    \n",
    "    Args:\n",
    "    - input_dir (str): Path to the NAM directory\n",
    "    - output_dir (str): Path to new directory to store filtered NAM data\n",
    "    Returns:\n",
    "    - None: Saves to the output directory.\n",
    "    \"\"\"\n",
    "    #Bounds to match up with uWRF (but a little higher)\n",
    "    #min_lat = 39.7\n",
    "    #max_lat = 43.2\n",
    "    #min_lon = -76.9\n",
    "    #max_lon = -72.5\n",
    "\n",
    "    #Bounds to match up with uWRF covering only NYC\n",
    "    min_lat = 40.380194091796874\n",
    "    max_lat = 41.090039825439455\n",
    "    min_lon = -74.34615478515624\n",
    "    max_lon = -73.495703125\n",
    "\n",
    "    for file_name in os.listdir(input_dir):\n",
    "        print(f\"Processing file: {file_name}\")\n",
    "        if file_name.endswith('.nc'):\n",
    "\n",
    "            file_path = os.path.join(input_dir, file_name)\n",
    "            dataset = xr.open_dataset(file_path)\n",
    "            \n",
    "            #Extract latitude and longitude variables\n",
    "            lat_var = 'latitude'\n",
    "            lon_var = 'longitude'\n",
    "            lat = dataset[lat_var].values\n",
    "            lon = dataset[lon_var].values\n",
    "            \n",
    "            #Filter the data based off of the spatial bounds\n",
    "            filtered_data = dataset.where(\n",
    "                (dataset[lat_var] >= min_lat) & (dataset[lat_var] <= max_lat) &\n",
    "                (dataset[lon_var] >= min_lon) & (dataset[lon_var] <= max_lon), drop=True)\n",
    "\n",
    "            output_file_path = os.path.join(output_dir, file_name)\n",
    "            filtered_data.to_netcdf(output_file_path)\n",
    "\n",
    "def main():\n",
    "    input_dir = \"/D4/data/gvaillant/NAM/2019/match-pred/03\"\n",
    "    output_dir = \"/D4/data/gvaillant/NAM/2019/match-NYC-cut/03\"\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    \n",
    "    NAM_spatial_filter(input_dir, output_dir)\n",
    "    \n",
    "    print(\"Done spatially filtering the NAM files!\")\n",
    "\n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "532ac977-c26a-451e-8fb9-96d3a8f1cb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: /D4/data/gvaillant/NAM/2019/match-NYC-final/train\n",
      "Saved combined dataset to: /D4/data/gvaillant/NAM/2019/match-NYC-final/train/NAM_final_01-02.nc\n",
      "Done combining the first two months of NAM files!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "\n",
    "def NAM_combine_seq(input_dirs, output_dir):\n",
    "    # Given two directories, combine files sequentially\n",
    "    combined_datasets = []\n",
    "    \n",
    "    for input_dir in input_dirs:\n",
    "        nc_files = sorted([os.path.join(input_dir, file) for file in os.listdir(input_dir) if file.endswith('.nc')])\n",
    "        datasets = [xr.open_dataset(nc_file) for nc_file in nc_files]\n",
    "        combined_datasets.extend(datasets)  # Add datasets sequentially\n",
    "\n",
    "    # Concatenate along the time dimension\n",
    "    combined_dataset = xr.concat(combined_datasets, dim='time')\n",
    "    \n",
    "    # Generate the output filename based on the directories' month identifiers\n",
    "    month_range = \"-\".join([os.path.basename(dir) for dir in input_dirs])\n",
    "    output_file_name = f'NAM_final_{month_range}.nc'\n",
    "    output_file_path = os.path.join(output_dir, output_file_name)\n",
    "    \n",
    "    # Save the combined dataset\n",
    "    combined_dataset.to_netcdf(output_file_path, encoding={'time': {'units': 'hours since 2019-1-1'}})\n",
    "    print(f\"Saved combined dataset to: {output_file_path}\")\n",
    "\n",
    "def main():\n",
    "\n",
    "    input_dirs = [\n",
    "        \"/D4/data/gvaillant/NAM/2019/match-NYC-cut/01\",\n",
    "        \"/D4/data/gvaillant/NAM/2019/match-NYC-cut/02\"\n",
    "    ]\n",
    "    \n",
    "    output_dir = \"/D4/data/gvaillant/NAM/2019/match-NYC-final/train\"\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    \n",
    "    NAM_combine_seq(input_dirs, output_dir)\n",
    "    \n",
    "    print(\"Done combining the first two months of NAM files!\")\n",
    "\n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d36dbb8-ed26-4980-9735-fd779782686d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: /D4/data/gvaillant/NAM/2019/match-NYC-final/test\n",
      "Saved combined dataset to: /D4/data/gvaillant/NAM/2019/match-NYC-final/test/NAM_final_03.nc\n",
      "Done combining the first half of the NAM files!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "\n",
    "def NAM_val_test(input_dirs, output_dir):\n",
    "    # Given two directories, combine files sequentially\n",
    "    combined_datasets = []\n",
    "    \n",
    "    for input_dir in input_dirs:\n",
    "        # Get sorted list of .nc files\n",
    "        nc_files = sorted([os.path.join(input_dir, file) for file in os.listdir(input_dir) if file.endswith('.nc')])\n",
    "        \n",
    "        # Take the first half of the files\n",
    "        half_length = len(nc_files) // 2\n",
    "        #selected_files = nc_files[:half_length]  # Select first half of the sorted files (val)\n",
    "        selected_files = nc_files[half_length:] #Select second half of the sorted files (test)\n",
    "        # Open the selected files\n",
    "        datasets = [xr.open_dataset(nc_file) for nc_file in selected_files]\n",
    "        combined_datasets.extend(datasets)  # Add datasets sequentially\n",
    "\n",
    "    # Concatenate along the time dimension\n",
    "    combined_dataset = xr.concat(combined_datasets, dim='time')\n",
    "    \n",
    "    # Generate the output filename based on the directories' month identifiers\n",
    "    month_range = \"-\".join([os.path.basename(dir) for dir in input_dirs])\n",
    "    output_file_name = f'NAM_final_{month_range}.nc'\n",
    "    output_file_path = os.path.join(output_dir, output_file_name)\n",
    "    \n",
    "    # Save the combined dataset\n",
    "    combined_dataset.to_netcdf(output_file_path, encoding={'time': {'units': 'hours since 2019-1-1'}})\n",
    "    print(f\"Saved combined dataset to: {output_file_path}\")\n",
    "\n",
    "def main():\n",
    "    input_dirs = [\n",
    "        \"/D4/data/gvaillant/NAM/2019/match-NYC-cut/03\"\n",
    "    ]\n",
    "    \n",
    "    output_dir = \"/D4/data/gvaillant/NAM/2019/match-NYC-final/test\"\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    \n",
    "    NAM_val_test(input_dirs, output_dir)\n",
    "    \n",
    "    print(\"Done combining the first half of the NAM files!\")\n",
    "\n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1cfb5f17-aba9-4591-8bda-18a41830ec67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "uwrf_train_data = xr.open_dataset('/D4/data/gvaillant/prep-uwrf/d02/pred-split/train/uWRF_final_01-02.nc')\n",
    "uwrf_val_data = xr.open_dataset('/D4/data/gvaillant/prep-uwrf/d02/pred-split/val/uWRF_final_03.nc')\n",
    "uwrf_test_data = xr.open_dataset('/D4/data/gvaillant/prep-uwrf/d02/pred-split/test/uWRF_final_03.nc')\n",
    "#--\n",
    "nam_train_data = xr.open_dataset('/D4/data/gvaillant/NAM/2019/pred-final/train/NAM_final_01-02.nc')\n",
    "nam_val_data = xr.open_dataset('/D4/data/gvaillant/NAM/2019/pred-final/val/NAM_final_03.nc')\n",
    "nam_test_data = xr.open_dataset('/D4/data/gvaillant/NAM/2019/pred-final/test/NAM_final_03.nc')\n",
    "\n",
    "nam_train_data = nam_train_data.rename({'PRES': 'PSFC'})\n",
    "nam_val_data = nam_val_data.rename({'PRES': 'PSFC'})\n",
    "nam_test_data = nam_test_data.rename({'PRES': 'PSFC'})\n",
    "\n",
    "# Function to align datasets\n",
    "def align_datasets(uwrf_data, nam_data):\n",
    "    # uWRF grid dimensions\n",
    "    uwrf_shape = uwrf_data.T2.shape  # Assuming T2 is representative of the shape\n",
    "    uwrf_lons = uwrf_data.longitude\n",
    "    uwrf_lats = uwrf_data.latitude\n",
    "\n",
    "    # Assign number of uWRF cells per NAM cell\n",
    "    uwrf_cells_per_lon = 4 #when using d03 we can use 12 bc we go from 12km to 1km\n",
    "    uwrf_cells_per_lat = 4\n",
    "\n",
    "    # Calculate new NAM grid dimensions\n",
    "    new_nam_lon_count = uwrf_shape[2] // uwrf_cells_per_lon\n",
    "    new_nam_lat_count = uwrf_shape[1] // uwrf_cells_per_lat\n",
    "\n",
    "    # Function to aggregate 4x4 uWRF cells into one NAM cell\n",
    "    def aggregate_4x4_grid(data):\n",
    "        reshaped = data.reshape(\n",
    "            data.shape[0],  # Time dimension remains unchanged\n",
    "            new_nam_lat_count, uwrf_cells_per_lat, \n",
    "            new_nam_lon_count, uwrf_cells_per_lon\n",
    "        )\n",
    "        aggregated = reshaped.mean(axis=(2, 4))  # Aggregate over latitude and longitude cells\n",
    "        return aggregated\n",
    "\n",
    "    # Determine the minimum time dimension between NAM and uWRF\n",
    "    min_time_steps = min(nam_data.time.size, uwrf_data.time.size)\n",
    "\n",
    "    # Slice both datasets to the same time dimension\n",
    "    nam_data_sliced = nam_data.isel(time=slice(0, min_time_steps))\n",
    "    uwrf_data_sliced = uwrf_data.isel(time=slice(0, min_time_steps))\n",
    "\n",
    "    # Initialize aligned data\n",
    "    aligned_data = {}\n",
    "\n",
    "    # Process both T2 and PRES\n",
    "    for var_name in ['T2', 'PSFC']:\n",
    "        if var_name in uwrf_data_sliced and var_name in nam_data_sliced:\n",
    "            uwrf_var = uwrf_data_sliced[var_name].values\n",
    "            aggregated_var = aggregate_4x4_grid(uwrf_var)\n",
    "            aligned_data[var_name] = (['time', 'latitude', 'longitude'], aggregated_var)\n",
    "        else:\n",
    "            raise ValueError(f\"Variable '{var_name}' not found in one of the datasets.\")\n",
    "\n",
    "    # Create a new dataset with aligned data\n",
    "    aligned_nam = xr.Dataset(\n",
    "        data_vars=aligned_data,\n",
    "        coords={\n",
    "            'time': nam_data_sliced.time,\n",
    "            'latitude': uwrf_lats[::uwrf_cells_per_lat][:new_nam_lat_count],\n",
    "            'longitude': uwrf_lons[::uwrf_cells_per_lon][:new_nam_lon_count]\n",
    "        },\n",
    "        attrs=nam_data.attrs\n",
    "    )\n",
    "\n",
    "    return aligned_nam\n",
    "\n",
    "#Uncomment the lines below to run the code:\n",
    "#aligned_nam_data = align_datasets(uwrf_test_data, nam_test_data)\n",
    "#aligned_nam_data.to_netcdf(\"/home/gvaillant1/aligned-data/aligned_nam_val_data.nc\")\n",
    "#print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b95fe6ef-c77f-475a-ad4b-bd98ebc10d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "## REGRIDDING NYC AREA:\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "#this the one i used\n",
    "uwrf_train_data = xr.open_dataset('/D4/data/gvaillant/prep-uwrf/d02/pred-NYC-split/train/uWRF_final_01-02.nc')\n",
    "uwrf_val_data = xr.open_dataset('/D4/data/gvaillant/prep-uwrf/d02/pred-NYC-split/val/uWRF_final_03.nc')\n",
    "uwrf_test_data = xr.open_dataset('/D4/data/gvaillant/prep-uwrf/d02/pred-NYC-split/test/uWRF_final_03.nc')\n",
    "#--\n",
    "nam_train_data = xr.open_dataset('/D4/data/gvaillant/NAM/2019/match-NYC-final/train/NAM_final_01-02.nc')\n",
    "nam_val_data = xr.open_dataset('/D4/data/gvaillant/NAM/2019/match-NYC-final/val/NAM_final_03.nc')\n",
    "nam_test_data = xr.open_dataset('/D4/data/gvaillant/NAM/2019/match-NYC-final/test/NAM_final_03.nc')\n",
    "\n",
    "nam_train_data = nam_train_data.rename({'PRES': 'PSFC'})\n",
    "nam_val_data = nam_val_data.rename({'PRES': 'PSFC'})\n",
    "nam_test_data = nam_test_data.rename({'PRES': 'PSFC'})\n",
    "\n",
    "# Function to align datasets\n",
    "def align_datasets(uwrf_data, nam_data):\n",
    "    # uWRF grid dimensions\n",
    "    uwrf_shape = uwrf_data.T2.shape  # Assuming T2 is representative of the shape\n",
    "    uwrf_lons = uwrf_data.longitude\n",
    "    uwrf_lats = uwrf_data.latitude\n",
    "\n",
    "    # Assign number of uWRF cells per NAM cell\n",
    "    uwrf_cells_per_lon = 4 #when using d03 we can use 12 bc we go from 12km to 1km\n",
    "    uwrf_cells_per_lat = 4\n",
    "\n",
    "    # Calculate new NAM grid dimensions\n",
    "    new_nam_lon_count = uwrf_shape[2] // uwrf_cells_per_lon\n",
    "    new_nam_lat_count = uwrf_shape[1] // uwrf_cells_per_lat\n",
    "\n",
    "    # Function to aggregate 4x4 uWRF cells into one NAM cell\n",
    "    def aggregate_4x4_grid(data):\n",
    "        reshaped = data.reshape(\n",
    "            data.shape[0],  # Time dimension remains unchanged\n",
    "            new_nam_lat_count, uwrf_cells_per_lat, \n",
    "            new_nam_lon_count, uwrf_cells_per_lon\n",
    "        )\n",
    "        aggregated = reshaped.mean(axis=(2, 4))  # Aggregate over latitude and longitude cells\n",
    "        return aggregated\n",
    "\n",
    "    # Determine the minimum time dimension between NAM and uWRF\n",
    "    min_time_steps = min(nam_data.time.size, uwrf_data.time.size)\n",
    "\n",
    "    # Slice both datasets to the same time dimension\n",
    "    nam_data_sliced = nam_data.isel(time=slice(0, min_time_steps))\n",
    "    uwrf_data_sliced = uwrf_data.isel(time=slice(0, min_time_steps))\n",
    "\n",
    "    # Initialize aligned data\n",
    "    aligned_data = {}\n",
    "\n",
    "    # Process both T2 and PRES\n",
    "    for var_name in ['T2', 'PSFC']:\n",
    "        if var_name in uwrf_data_sliced and var_name in nam_data_sliced:\n",
    "            uwrf_var = uwrf_data_sliced[var_name].values\n",
    "            aggregated_var = aggregate_4x4_grid(uwrf_var)\n",
    "            aligned_data[var_name] = (['time', 'latitude', 'longitude'], aggregated_var)\n",
    "        else:\n",
    "            raise ValueError(f\"Variable '{var_name}' not found in one of the datasets.\")\n",
    "\n",
    "    # Create a new dataset with aligned data\n",
    "    aligned_nam = xr.Dataset(\n",
    "        data_vars=aligned_data,\n",
    "        coords={\n",
    "            'time': nam_data_sliced.time,\n",
    "            'latitude': uwrf_lats[::uwrf_cells_per_lat][:new_nam_lat_count],\n",
    "            'longitude': uwrf_lons[::uwrf_cells_per_lon][:new_nam_lon_count]\n",
    "        },\n",
    "        attrs=nam_data.attrs\n",
    "    )\n",
    "\n",
    "    return aligned_nam\n",
    "\n",
    "#Uncomment the lines below to run the code:\n",
    "#aligned_nam_data = align_datasets(uwrf_test_data, nam_test_data)\n",
    "#aligned_nam_data.to_netcdf(\"/home/gvaillant1/aligned-NYC-data/aligned_nam_test_data.nc\")\n",
    "#print(\"Done\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
