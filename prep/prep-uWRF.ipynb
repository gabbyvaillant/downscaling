{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034e15de-af30-485f-a1a2-343654250068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from scipy.interpolate import griddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5059551-c07e-4984-b9ee-9d2b9d9af87f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def uWRF_filter_vars(input_dir, output_dir, variables):\n",
    "\n",
    "    input_files = glob.glob(os.path.join(input_dir, '*'))\n",
    "    \n",
    "    for file in input_files:\n",
    "        ds = xr.open_dataset(file)\n",
    "        ds_filtered = ds[variables]\n",
    "\n",
    "        #Rename specific variables\n",
    "        ds_filtered = ds_filtered.rename({'XLAT': 'latitude', 'XLONG': 'longitude', 'XTIME': 'time'})\n",
    "\n",
    "        filename = os.path.basename(file)\n",
    "        output_file = os.path.join(output_dir, filename)\n",
    "        print(f\"Saving file to: {output_file}\")\n",
    "        ds_filtered.to_netcdf(output_file)\n",
    "       \n",
    "def main():\n",
    "    \n",
    "    for i in range(1, 32):\n",
    "        remote_input_dir = f\"/D4/data/gvaillant/uwrf/01/{str(i).zfill(2)}/d02_files\" #CHANGED TO DOMAIN 2 FOR ONLY SPATIAL DOWNSCALING\n",
    "        print(f\"Processing directory: {remote_input_dir}\")\n",
    "        \n",
    "        remote_output_dir = f\"/D4/data/gvaillant/prep-uwrf/12stage1/01/{str(i).zfill(2)}\"\n",
    "        print(f\"Output directory: {remote_output_dir}\")\n",
    "        \n",
    "        uWRF_filter_vars(remote_input_dir, remote_output_dir, ['T2'])\n",
    "            \n",
    "    print(\"Done processing stage1 uWRF files!\")\n",
    "\n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e8002b1-a1a8-4120-85b5-a172f7004dc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def uWRF_match_dims(input_dir, output_dir):\n",
    "    \n",
    "    input_files = glob.glob(os.path.join(input_dir, '*'))\n",
    "    \n",
    "    for file_name in input_files:\n",
    "        \n",
    "        print(f\"Processing file: {file_name}\")\n",
    "        ds = xr.open_dataset(file_name)\n",
    "\n",
    "        lat_values = ds['latitude'].values \n",
    "        lon_values = ds['longitude'].values \n",
    "        time = ds['time']\n",
    "        \n",
    "        latitudes = lat_values[0,:,:] #take all the lat values from the first time step\n",
    "        longitudes = lon_values[0,:,:] #take all of the lon values from the first time step\n",
    "        \n",
    "        lat_attrs = ds['latitude'].attrs\n",
    "        lon_attrs = ds['longitude'].attrs\n",
    "        time_attrs = ds['time'].attrs\n",
    "        \n",
    "        #Flatten latitude and longitude for interpolation\n",
    "        #zip gets pairs and flatten makes them 1d arrays\n",
    "        points = np.array([(lon, lat) for lat, lon in zip(latitudes.flatten(), longitudes.flatten())])\n",
    "        \n",
    "        #Define the new latitude and longitude grid\n",
    "        new_latitudes = np.linspace(np.min(latitudes), np.max(latitudes), num=latitudes.shape[0])\n",
    "        new_longitudes = np.linspace(np.min(longitudes), np.max(longitudes), num=longitudes.shape[1])\n",
    "        \n",
    "        new_lon_grid, new_lat_grid = np.meshgrid(new_longitudes, new_latitudes)\n",
    "        \n",
    "        new_vars = {}\n",
    "        \n",
    "        for var_name in ds.data_vars:\n",
    "            var = ds[var_name]\n",
    "            new_var_list = []\n",
    "            \n",
    "            for t in range(var.sizes['Time']):\n",
    "                weather_variable = var.isel(Time=t).values  # Extract the data for the time step\n",
    "                \n",
    "                #Flatten the weather variable data\n",
    "                values = weather_variable.flatten()\n",
    "                \n",
    "                #Interpolate the data onto the new grid\n",
    "                new_weather_variable = griddata(points, values, (new_lon_grid, new_lat_grid), method='linear')\n",
    "                \n",
    "                #Append the interpolated data for the current time step\n",
    "                new_var_list.append(new_weather_variable)\n",
    "            \n",
    "            new_vars[var_name] = (['time', 'latitude', 'longitude'], np.stack(new_var_list))\n",
    "        \n",
    "        new_ds = xr.Dataset(\n",
    "            new_vars, coords={'latitude': new_latitudes,\n",
    "                              'longitude': new_longitudes,\n",
    "                              'time': ds['time'].values})\n",
    "        \n",
    "        new_ds['latitude'].attrs.update(lat_attrs)\n",
    "        new_ds['longitude'].attrs.update(lon_attrs)\n",
    "        new_ds['time'].attrs.update(time_attrs)\n",
    "        \n",
    "        for var_name in ds.data_vars:\n",
    "            new_ds[var_name].attrs.update(ds[var_name].attrs)\n",
    "            \n",
    "        #Drop the 'Time' dimension\n",
    "        if 'Time' in new_ds.dims:\n",
    "            new_ds = new_ds.drop_dims('Time')\n",
    "\n",
    "        new_ds.attrs.update(ds.attrs)\n",
    "        \n",
    "        output_file_name = os.path.basename(file_name)\n",
    "        output_file_path = os.path.join(output_dir, output_file_name)\n",
    "        new_ds.to_netcdf(output_file_path)\n",
    "\n",
    "\n",
    "def main():\n",
    "    for i in range(1, 32):\n",
    "        input_dir = f\"/D4/data/gvaillant/prep-uwrf/d02/stage1/01/{str(i).zfill(2)}\"\n",
    "        print(f\"Processing directory: {input_dir}\")\n",
    "\n",
    "        output_dir = f\"/D4/data/gvaillant/prep-uwrf/d02/stage2/01/{str(i).zfill(2)}\"\n",
    "        print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "        uWRF_match_dims(input_dir, output_dir)\n",
    "        print(f\"Done processing directory: /D3/data/gvaillant/prep-uwrf/d02/stage1/01/{str(i).zfill(2)}\")\n",
    "        \n",
    "    print(f\"Done processing stage2 uWRF files!\")\n",
    "\n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cdfc100-6659-48d6-bc4b-68fdb52f6c4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def uWRF_spatial_filtering(input_dir, output_dir):\n",
    "\n",
    "    #Bounds to cover Manhattan\n",
    "    #May have to change later\n",
    "    min_lat = 40.533801\n",
    "    max_lat = 40.955109\n",
    "    min_lon = -74.131557\n",
    "    max_lon = -73.762832\n",
    "\n",
    "    \"\"\"\n",
    "    min_lat = 40.57384924257281\n",
    "    max_lat = 40.92\n",
    "    min_lon = -74.0481110602903\n",
    "    max_lon = -73.84627819243957\n",
    "    \"\"\"\n",
    "    input_files = glob.glob(os.path.join(input_dir, '*')) #* = Not specifying file format\n",
    "    \n",
    "    for file in input_files:\n",
    "        print(f\"Processing file: {file}\")\n",
    "        ds = xr.open_dataset(file)\n",
    "\n",
    "        #Extract latitude and longitude variables\n",
    "        lat_var = 'latitude'\n",
    "        lon_var = 'longitude'\n",
    "        lat = ds[lat_var].values\n",
    "        lon = ds[lon_var].values\n",
    "\n",
    "        #Filter the data based off of the spatial bounds\n",
    "        filtered_data = ds.where(\n",
    "            (ds[lat_var] >= min_lat) & (ds[lat_var] <= max_lat) &\n",
    "            (ds[lon_var] >= min_lon) & (ds[lon_var] <= max_lon), drop=True)\n",
    "\n",
    "        \n",
    "        filename = os.path.basename(file)\n",
    "        output_file_path = os.path.join(output_dir, filename)\n",
    "        filtered_data.to_netcdf(output_file_path)\n",
    "\n",
    "def main():\n",
    "\n",
    "    for i in range(1, 32):\n",
    "        \n",
    "        input_dir = f\"/D4/data/gvaillant/prep-uwrf/stage2/01/{str(i).zfill(2)}\" \n",
    "        print(f\"Processing directory: {input_dir}\")\n",
    "        \n",
    "        output_dir = f\"/D4/data/gvaillant/prep-uwrf/12stage3/01/{str(i).zfill(2)}\"\n",
    "        print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "        uWRF_spatial_filtering(input_dir, output_dir)\n",
    "        print(f\"Done processing directory: /D3/data/gvaillant/prep-uwrf/stage2/01/{str(i).zfill(2)}\")\n",
    "        \n",
    "    print(\"Done processing stage3 uWRF files!\")\n",
    "        \n",
    "\n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88beb7df-7e1f-4b63-bb16-3eeee3a2f6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uWRF_combine_seq(input_dirs, output_dir):\n",
    "    combined_datasets = []\n",
    "    \n",
    "    for input_dir in input_dirs:\n",
    "        # Loop through each day's subdirectory within the monthly directory\n",
    "        day_dirs = sorted(next(os.walk(input_dir))[1])  # Get list of day subdirectories\n",
    "        for day_dir in day_dirs:\n",
    "            day_path = os.path.join(input_dir, day_dir)  # Full path to each day's subdirectory\n",
    "            input_files = sorted(glob.glob(os.path.join(day_path, '*')))  # Gather all files in the daily subdirectory\n",
    "            \n",
    "            # Load each file into a dataset and add to the list\n",
    "            datasets = [xr.open_dataset(file) for file in input_files]\n",
    "            combined_datasets.extend(datasets)\n",
    "\n",
    "    # Concatenate all datasets along the 'time' dimension\n",
    "    combined_dataset = xr.concat(combined_datasets, dim='time')\n",
    "\n",
    "    # Generate output filename based on month range from input directories\n",
    "    month_range = \"-\".join([os.path.basename(month_dir) for month_dir in input_dirs])\n",
    "    output_file_name = f'uWRF_final_{month_range}.nc'\n",
    "    output_file_path = os.path.join(output_dir, output_file_name)\n",
    "\n",
    "    # Save the concatenated dataset to a NetCDF file\n",
    "    combined_dataset.to_netcdf(output_file_path)\n",
    "    print(f'Combined dataset saved to {output_file_path}')\n",
    "\n",
    "def main():\n",
    "    input_dirs = [\n",
    "        \"/D4/data/gvaillant/prep-uwrf/d02/pred-stage2/01\",\n",
    "        \"/D4/data/gvaillant/prep-uwrf/d02/pred-stage2/02\"\n",
    "    ]\n",
    "    output_dir = '/D4/data/gvaillant/prep-uwrf/d02/pred-split/train'\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "    uWRF_combine_seq(input_dirs, output_dir)\n",
    "    print(\"Done combining uWRF files!\")\n",
    "\n",
    "\n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0145a082-4c06-4dfc-aba6-8bb20806a109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uWRF_val_test(input_dirs, output_dir):\n",
    "    combined_datasets = []\n",
    "    \n",
    "    for input_dir in input_dirs:\n",
    "        #Loop through each day's subdirectory within the monthly directory\n",
    "        day_dirs = sorted(next(os.walk(input_dir))[1])  # Get list of day subdirectories\n",
    "        for day_dir in day_dirs:\n",
    "            day_path = os.path.join(input_dir, day_dir)  # Full path to each day's subdirectory\n",
    "            nc_files = sorted(glob.glob(os.path.join(day_path, '*')))  # Gather all files in the daily subdirectory\n",
    "\n",
    "            #Take the first half of the files OR second half\n",
    "            half_length = len(nc_files) // 2\n",
    "            #selected_files = nc_files[:half_length]  #EDIT: Select first half of the sorted files (VALIDATION)\n",
    "            selected_files = nc_files[half_length:] #EDIT: Select second half of the sorted files (TESTING)\n",
    "            \n",
    "            datasets = [xr.open_dataset(file) for file in selected_files]\n",
    "            combined_datasets.extend(datasets)\n",
    "\n",
    "    #Concatenate all datasets along the 'time' dimension\n",
    "    combined_dataset = xr.concat(combined_datasets, dim='time')\n",
    "\n",
    "    #Generate output filename based on month range from input directories\n",
    "    month_range = \"-\".join([os.path.basename(month_dir) for month_dir in input_dirs])\n",
    "    output_file_name = f'uWRF_final_{month_range}.nc'\n",
    "    output_file_path = os.path.join(output_dir, output_file_name)\n",
    "\n",
    "    combined_dataset.to_netcdf(output_file_path)\n",
    "    print(f'Combined dataset saved to {output_file_path}')\n",
    "\n",
    "def main():\n",
    "    input_dirs = [\n",
    "        \"/D4/data/gvaillant/prep-uwrf/d02/pred-stage2/03\"\n",
    "    ]\n",
    "    output_dir = '/D4/data/gvaillant/prep-uwrf/d02/pred-split/test' #EDIT: Change to test or val\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "    uWRF_combine_seq(input_dirs, output_dir)\n",
    "    print(\"Done combining uWRF files!\")\n",
    "\n",
    "\n",
    "#main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
