{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "984c2d9d-bdec-4adb-8502-8e9cb6701cfa",
   "metadata": {},
   "source": [
    "# Preprocessing Functions used to prepare data for Downscaling Wind Speed\n",
    "\n",
    "In order to downscale wind speed, we need the same variable present in both the NAM and uWRF datasets. \n",
    "\n",
    "We are mostly interested in wind speed at the height of wind turbines. Only in the NAM dataset we have the wind speed for that altitude. The uWRF dataset does not provide that information. Instead, we will use the U and V components of wind (to calulcate the wind speed) at 10 meters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba653282-ddfe-467d-a164-2abdf209079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from scipy.interpolate import griddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e52ffe7f-c87a-461d-9cb2-1f5a2e16132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uWRF_filter_vars_with_pred(input_dir, output_dir, variables):\n",
    "\n",
    "    input_files = glob.glob(os.path.join(input_dir, '*'))\n",
    "    \n",
    "    for file in input_files:\n",
    "        \n",
    "        print(f\"Processing file: {file}\")\n",
    "        ds = xr.open_dataset(file)\n",
    "        \n",
    "        #Keep variable choosen by the user. This should be the variable being downscaled and an associated predictor variable\n",
    "        ds_filtered = ds[variables]\n",
    "\n",
    "        #Rename variables\n",
    "        ds_filtered = ds_filtered.rename({'XLAT': 'latitude', 'XLONG': 'longitude', 'XTIME': 'time'})\n",
    "\n",
    "        filename = os.path.basename(file)\n",
    "        output_file = os.path.join(output_dir, filename)\n",
    "        print(f\"Saving file to: {output_file}\")\n",
    "        ds_filtered.to_netcdf(output_file)\n",
    "       \n",
    "def main():\n",
    "    \n",
    "    for i in range(1, 32):\n",
    "        remote_input_dir = f\"/D4/data/gvaillant/uwrf/03/{str(i).zfill(2)}/d02_files\" #Use either d02 files or d03 files\n",
    "        print(f\"Processing directory: {remote_input_dir}\")\n",
    "        \n",
    "        remote_output_dir = f\"/D4/data/gvaillant/prep-uwrf/d02/wind-stage1/03/{str(i).zfill(2)}\"\n",
    "        print(f\"Output directory: {remote_output_dir}\")\n",
    "        \n",
    "        uWRF_filter_vars_with_pred(remote_input_dir, remote_output_dir, ['U10', 'V10', 'PSFC']) #User chooses the variables \n",
    "            \n",
    "    print(\"Done processing stage1 uWRF files with predictor!\")\n",
    "\n",
    "#Uncomment below to run:\n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f22bd9ac-4edb-42c8-95ae-0b1ef606d55c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "def wind_speed(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Calculates wind speed from U10 and V10 components for NetCDF files in the input directory,\n",
    "    adds it as a new variable ('WS'), keeps only PSFC and WS, and saves the modified files to the output directory.\n",
    "\n",
    "    Parameters:\n",
    "        input_dir (str): Path to the directory containing input NetCDF files.\n",
    "        output_dir (str): Path to the directory to save the modified NetCDF files.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get list of all files in the input directory\n",
    "    input_files = glob.glob(os.path.join(input_dir, '*'))\n",
    "\n",
    "    for file_name in input_files:\n",
    "        print(f\"Processing file: {file_name}\")\n",
    "        try:\n",
    "            # Open the NetCDF file\n",
    "            ds = xr.open_dataset(file_name)\n",
    "\n",
    "            # Calculate wind speed\n",
    "            u = ds['U10'].data  # Extract data as numpy array\n",
    "            v = ds['V10'].data  # Extract data as numpy array\n",
    "            wind_speed = np.sqrt(u**2 + v**2)\n",
    "\n",
    "            # Add wind speed as a new variable to the dataset\n",
    "            ds['WS'] = (('Time', 'south_north', 'west_east'), wind_speed)\n",
    "            ds['WS'].attrs['units'] = 'm/s'\n",
    "            ds['WS'].attrs['description'] = 'Calculated wind speed from U10 and V10'\n",
    "\n",
    "            # Keep only PSFC and WS\n",
    "            new_ds = ds[['PSFC', 'WS']]\n",
    "\n",
    "            # Save the modified dataset to the output directory\n",
    "            output_file_name = os.path.basename(file_name)\n",
    "            output_file_path = os.path.join(output_dir, output_file_name)\n",
    "            new_ds.to_netcdf(output_file_path)\n",
    "            print(f\"Saved file to: {output_file_path}\")\n",
    "\n",
    "            # Close the dataset\n",
    "            ds.close()\n",
    "            new_ds.close()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_name}: {e}\")\n",
    "\n",
    "def main():\n",
    "    for i in range(31, 32):\n",
    "        input_dir = f\"/D4/data/gvaillant/prep-uwrf/d02/wind-stage1/01/{str(i).zfill(2)}\"\n",
    "        print(f\"Processing directory: {input_dir}\")\n",
    "\n",
    "        output_dir = f\"/D4/data/gvaillant/prep-uwrf/d02/windspeed/01/{str(i).zfill(2)}\"\n",
    "        print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "        wind_speed(input_dir, output_dir)\n",
    "        print(f\"Done processing directory: {input_dir}\")\n",
    "\n",
    "    print(\"Done adding wind speed variable to uWRF files!\")\n",
    "\n",
    "#Uncomment below to run:\n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "435d0e04-487c-453f-b02f-2e09ca2f5ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uWRF_match_dims_with_pred(input_dir, output_dir):\n",
    "    \n",
    "    input_files = glob.glob(os.path.join(input_dir, '*'))\n",
    "\n",
    "    for file_name in input_files:\n",
    "        \n",
    "        print(f\"Processing file: {file_name}\")\n",
    "        ds = xr.open_dataset(file_name)\n",
    "\n",
    "        #Extract latitude, longitude, and time values (these will be the dimensions of the dataset)\n",
    "        lat_values = ds['latitude'].values[0, :, :]  # Use the first timestep for latitudes *CHECKKKKKKK*\n",
    "        lon_values = ds['longitude'].values[0, :, :]  # Use the first timestep for longitudes\n",
    "        time = ds['time']\n",
    "\n",
    "        #Preserve attributes\n",
    "        lat_attrs = ds['latitude'].attrs\n",
    "        lon_attrs = ds['longitude'].attrs\n",
    "        time_attrs = ds['time'].attrs\n",
    "\n",
    "        #Reorganize dataset dimensions\n",
    "        new_vars = {}\n",
    "        for var_name in ds.data_vars:\n",
    "            var = ds[var_name]\n",
    "            new_vars[var_name] = (['time', 'latitude', 'longitude'], var.values)  #Reassign dimensions\n",
    "\n",
    "        #Create a new dataset with updated dimensions\n",
    "        new_ds = xr.Dataset(\n",
    "            new_vars,\n",
    "            coords={\n",
    "                'latitude': (['latitude'], lat_values[:, 0]),  # Convert to 1D\n",
    "                'longitude': (['longitude'], lon_values[0, :]),  # Convert to 1D\n",
    "                'time': time.values\n",
    "            }\n",
    "        )\n",
    "\n",
    "        #Add attributes\n",
    "        new_ds['latitude'].attrs.update(lat_attrs)\n",
    "        new_ds['longitude'].attrs.update(lon_attrs)\n",
    "        new_ds['time'].attrs.update(time_attrs)\n",
    "\n",
    "        #Add variable attributes\n",
    "        for var_name in ds.data_vars:\n",
    "            new_ds[var_name].attrs.update(ds[var_name].attrs)\n",
    "\n",
    "        #Add global attributes\n",
    "        new_ds.attrs.update(ds.attrs)\n",
    "\n",
    "        output_file_name = os.path.basename(file_name)\n",
    "        output_file_path = os.path.join(output_dir, output_file_name)\n",
    "        new_ds.to_netcdf(output_file_path)\n",
    "        print(f\"Saved file to: {output_file_path}\")\n",
    "\n",
    "    print(\"Done with uWRF dimension adjustment!\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    for i in range(1, 32):\n",
    "        input_dir = f\"/D4/data/gvaillant/prep-uwrf/d02/windspeed/03/{str(i).zfill(2)}\"\n",
    "        print(f\"Processing directory: {input_dir}\")\n",
    "\n",
    "        output_dir = f\"/D4/data/gvaillant/prep-uwrf/d02/wind-stage2/03/{str(i).zfill(2)}\"\n",
    "        print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "        uWRF_match_dims_with_pred(input_dir, output_dir)\n",
    "        print(f\"Done processing directory: {input_dir}\")\n",
    "\n",
    "    print(\"Done processing stage2 uWRF files!\")\n",
    "\n",
    "#Uncomment below to run:\n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a8174b2d-523f-4846-9ba0-70b12a62bb69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def adjust_bounds_to_match(lat_values, lon_values, min_lat, max_lat, min_lon, max_lon):\n",
    "    \"\"\"\n",
    "    Adjust bounds until the number of latitude and longitude values is the same,\n",
    "    neither is a prime number, and both are divisible by 4.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        lat_count = len(lat_values[(lat_values >= min_lat) & (lat_values <= max_lat)])\n",
    "        lon_count = len(lon_values[(lon_values >= min_lon) & (lon_values <= max_lon)])\n",
    "\n",
    "        # Check if BOTH conditions are met\n",
    "        if lat_count == lon_count and lat_count % 4 == 0:\n",
    "            break  # Conditions satisfied: same count, not prime, divisible by 4\n",
    "\n",
    "        # Adjust bounds to change the count\n",
    "        if lat_count != lon_count:\n",
    "            if lat_count < lon_count:\n",
    "                max_lat += (lat_values[1] - lat_values[0])  # Increment latitude bound\n",
    "            else:\n",
    "                max_lon += (lon_values[1] - lon_values[0])  # Increment longitude bound\n",
    "        else:  # If counts are equal but not meeting the conditions\n",
    "            max_lat += (lat_values[1] - lat_values[0])  # Increment latitude bound\n",
    "            max_lon += (lon_values[1] - lon_values[0])  # Increment longitude bound\n",
    "\n",
    "    return min_lat, max_lat, min_lon, max_lon\n",
    "\n",
    "\n",
    "def uWRF_spatial_cut(input_dir, output_dir, min_lat, max_lat, min_lon, max_lon):\n",
    "    \"\"\"\n",
    "    Function to spatially filter uWRF data to cover NYC, the boroughs, and ensure \n",
    "    consistent latitude and longitude dimensions, avoiding prime numbers and ensuring divisibility by 4.\n",
    "    \"\"\"\n",
    "    input_files = glob.glob(os.path.join(input_dir, '*'))\n",
    "\n",
    "    for file in input_files:\n",
    "        print(f\"Processing file: {file}\")\n",
    "        ds = xr.open_dataset(file)\n",
    "\n",
    "        lat = ds['latitude']\n",
    "        lon = ds['longitude']\n",
    "\n",
    "        # Handle 1D or 2D coordinates dynamically\n",
    "        if lat.ndim == 1 and lon.ndim == 1:\n",
    "            # 1D Coordinates\n",
    "            lat_values = lat.values\n",
    "            lon_values = lon.values\n",
    "\n",
    "            # Adjust bounds to match dimensions and avoid prime numbers\n",
    "            min_lat, max_lat, min_lon, max_lon = adjust_bounds_to_match(\n",
    "                lat_values, lon_values, min_lat, max_lat, min_lon, max_lon\n",
    "            )\n",
    "\n",
    "            filtered_data = ds.sel(\n",
    "                latitude=slice(min_lat, max_lat),\n",
    "                longitude=slice(min_lon, max_lon)\n",
    "            )\n",
    "        else:\n",
    "            # 2D Coordinates\n",
    "            lat_mask = (lat >= min_lat) & (lat <= max_lat)\n",
    "            lon_mask = (lon >= min_lon) & (lon <= max_lon)\n",
    "            combined_mask = lat_mask & lon_mask\n",
    "\n",
    "            filtered_data = ds.where(combined_mask, drop=True)\n",
    "\n",
    "            # Ensure lat/lon counts are equal, not prime, and divisible by 4\n",
    "            lat_values = filtered_data['latitude'].values\n",
    "            lon_values = filtered_data['longitude'].values\n",
    "            min_lat, max_lat, min_lon, max_lon = adjust_bounds_to_match(\n",
    "                lat_values, lon_values, min_lat, max_lat, min_lon, max_lon\n",
    "            )\n",
    "\n",
    "        # Save the output\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        filename = os.path.basename(file)\n",
    "        output_file = os.path.join(output_dir, filename)\n",
    "        print(f\"Saving file to: {output_file}\")\n",
    "        filtered_data.to_netcdf(output_file)\n",
    "\n",
    "\n",
    "def main():\n",
    "    for i in range(1, 32):\n",
    "        input_dir = f\"/D4/data/gvaillant/prep-uwrf/d02/wind-stage2/03/{str(i).zfill(2)}\"\n",
    "        print(f\"Processing directory: {input_dir}\")\n",
    "\n",
    "        output_dir = f\"/D4/data/gvaillant/prep-uwrf/d02/wind-stage3/03/{str(i).zfill(2)}\"\n",
    "        print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "        # Bounds to cover NYC and Boroughs\n",
    "        min_lat = 40.4774\n",
    "        max_lat = 40.9176\n",
    "        min_lon = -74.2591\n",
    "        max_lon = -73.7004\n",
    "\n",
    "        uWRF_spatial_cut(input_dir, output_dir, min_lat, max_lat, min_lon, max_lon)\n",
    "        print(f\"Done spatially filtering the uWRF files to NYC!\")\n",
    "\n",
    "#Uncomment below to run:\n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1febdbf0-b6d8-4f82-acc0-6a6d0ed5066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import xarray as xr\n",
    "\n",
    "def uWRF_combine_seq(input_dirs, output_dir):\n",
    "    combined_datasets = []\n",
    "    \n",
    "    for input_dir in input_dirs:\n",
    "        # Loop through each day's subdirectory within the monthly directory\n",
    "        day_dirs = sorted(next(os.walk(input_dir))[1])  # Get list of day subdirectories\n",
    "        for day_dir in day_dirs:\n",
    "            day_path = os.path.join(input_dir, day_dir)  # Full path to each day's subdirectory\n",
    "            input_files = sorted(glob.glob(os.path.join(day_path, '*')))  # Gather all files in the daily subdirectory\n",
    "            \n",
    "            # Load each file into a dataset and add to the list\n",
    "            datasets = [xr.open_dataset(file) for file in input_files]\n",
    "            combined_datasets.extend(datasets)\n",
    "\n",
    "    # Concatenate all datasets along the 'time' dimension\n",
    "    combined_dataset = xr.concat(combined_datasets, dim='time')\n",
    "\n",
    "    # Generate output filename based on month range from input directories\n",
    "    month_range = \"-\".join([os.path.basename(month_dir) for month_dir in input_dirs])\n",
    "    output_file_name = f'uWRF_final_{month_range}.nc'\n",
    "    output_file_path = os.path.join(output_dir, output_file_name)\n",
    "\n",
    "    # Save the concatenated dataset to a NetCDF file\n",
    "    combined_dataset.to_netcdf(output_file_path, encoding={'time': {'units': 'hours since 2019-10-11'}}) #that isnt correct\n",
    "    print(f'Combined dataset saved to {output_file_path}')\n",
    "\n",
    "def main():\n",
    "    input_dirs = [\n",
    "        \"/D4/data/gvaillant/prep-uwrf/d02/wind-stage3/01\",\n",
    "        \"/D4/data/gvaillant/prep-uwrf/d02/wind-stage3/02\"\n",
    "    ]\n",
    "    output_dir = '/D4/data/gvaillant/prep-uwrf/d02/wind-NYC-split/train'\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "    uWRF_combine_seq(input_dirs, output_dir)\n",
    "    print(\"Done combining uWRF files!\")\n",
    "\n",
    "#Uncomment below to run:\n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "39c55042-1402-4469-a20c-1238713ecc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: /D4/data/gvaillant/prep-uwrf/d02/wind-NYC-split/val\n",
      "Combined dataset saved to /D4/data/gvaillant/prep-uwrf/d02/wind-NYC-split/val/uWRF_final_03.nc\n",
      "Done combining uWRF files!\n"
     ]
    }
   ],
   "source": [
    "def uWRF_val_test(input_dirs, output_dir):\n",
    "    combined_datasets = []\n",
    "    \n",
    "    for input_dir in input_dirs:\n",
    "        # Loop through each day's subdirectory within the monthly directory\n",
    "        day_dirs = sorted(next(os.walk(input_dir))[1])  # Get list of day subdirectories\n",
    "        for day_dir in day_dirs:\n",
    "            day_path = os.path.join(input_dir, day_dir)  # Full path to each day's subdirectory\n",
    "            nc_files = sorted(glob.glob(os.path.join(day_path, '*')))  # Gather all files in the daily subdirectory\n",
    "\n",
    "            # Take the first half of the files\n",
    "            half_length = len(nc_files) // 2\n",
    "            selected_files = nc_files[:half_length]  # Select first half of the sorted files (VALIDATION)\n",
    "            #selected_files = nc_files[half_length:] #Select second half of the sorted files (TESTING)\n",
    "            \n",
    "            # Open the selected files\n",
    "            # Load each file into a dataset and add to the list\n",
    "            datasets = [xr.open_dataset(file) for file in selected_files]\n",
    "            combined_datasets.extend(datasets)\n",
    "\n",
    "    # Concatenate all datasets along the 'time' dimension\n",
    "    combined_dataset = xr.concat(combined_datasets, dim='time')\n",
    "\n",
    "    # Generate output filename based on month range from input directories\n",
    "    month_range = \"-\".join([os.path.basename(month_dir) for month_dir in input_dirs])\n",
    "    output_file_name = f'uWRF_final_{month_range}.nc'\n",
    "    output_file_path = os.path.join(output_dir, output_file_name)\n",
    "\n",
    "    # Save the concatenated dataset to a NetCDF file\n",
    "    combined_dataset.to_netcdf(output_file_path, encoding={'time': {'units': 'hours since 2019-10-11'}})\n",
    "    print(f'Combined dataset saved to {output_file_path}')\n",
    "\n",
    "def main():\n",
    "    input_dirs = [\n",
    "        \"/D4/data/gvaillant/prep-uwrf/d02/wind-stage3/03\"\n",
    "    ]\n",
    "    output_dir = '/D4/data/gvaillant/prep-uwrf/d02/wind-NYC-split/val' #change to val or test\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "    uWRF_combine_seq(input_dirs, output_dir)\n",
    "    print(\"Done combining uWRF files!\")\n",
    "\n",
    "#Uncomment below to run:\n",
    "#main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed08c1a-ec59-4ee9-a430-547a3159717b",
   "metadata": {},
   "source": [
    "# NAM functions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9056e10-cb54-4ea4-a3a8-0632fc54a04b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Fix this one for wind speed\n",
    "import netCDF4\n",
    "import xarray as xr\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "def NAM_filter_and_match_dims(input_dir, output_dir, variables):\n",
    "    # List all files in input directory\n",
    "    file_list = [(os.path.join(input_dir, file), file) for file in os.listdir(input_dir) if file.endswith('.nc')]\n",
    "\n",
    "    for file_path, file_name in file_list:  # Unpack the tuple\n",
    "        try:\n",
    "            # Step 1: Filter the variables using NAM_filter_vars logic\n",
    "            with xr.open_dataset(file_path) as ds:\n",
    "                existing_vars = {var: ds[var] for var in variables.keys() if var in ds}\n",
    "                if not existing_vars:\n",
    "                    print(f\"No matching variables found in {file_name}.\")\n",
    "                    continue\n",
    "\n",
    "                # Filter and rename variables\n",
    "                ds_filtered = xr.Dataset(existing_vars).rename(variables)\n",
    "\n",
    "                for var in ds_filtered.data_vars:\n",
    "                    if 'time' in ds_filtered[var].dims:\n",
    "                        dims = ('time',) + tuple(d for d in ds_filtered[var].dims if d != 'time')\n",
    "                        ds_filtered[var] = ds_filtered[var].transpose(*dims)\n",
    "\n",
    "                for orig_var, new_var in variables.items():\n",
    "                    if orig_var in ds:\n",
    "                        ds_filtered[new_var].attrs = ds[orig_var].attrs\n",
    "\n",
    "                ds_filtered.attrs = ds.attrs\n",
    "\n",
    "                # Change longitude values to be in degrees west\n",
    "                if 'longitude' in ds_filtered:\n",
    "                    lon = ds_filtered['longitude'].values\n",
    "                    lon = np.where(lon > 180, lon - 360, lon)\n",
    "                    ds_filtered['longitude'].values = lon\n",
    "                    ds_filtered['longitude'].attrs['units'] = 'degrees_west'\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_name}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Step 2: Interpolate variables using NAM_match_dims logic\n",
    "        latitudes = ds_filtered['latitude'].values  # Shape: (67, 71)\n",
    "        longitudes = ds_filtered['longitude'].values  # Shape: (67, 71)\n",
    "        time = ds_filtered['time']\n",
    "        \n",
    "        # Save all the attributes for each variable\n",
    "        lat_attrs = ds_filtered['latitude'].attrs\n",
    "        lon_attrs = ds_filtered['longitude'].attrs\n",
    "        time_attrs = ds_filtered['time'].attrs\n",
    "        \n",
    "        # Flatten latitude and longitude for interpolation\n",
    "        points = np.array([(lon, lat) for lat_row, lon_row in zip(latitudes, longitudes) for lat, lon in zip(lat_row, lon_row)])\n",
    "        \n",
    "        # Define the new latitude and longitude grid\n",
    "        new_latitudes = np.linspace(np.min(latitudes), np.max(latitudes), num=67)\n",
    "        new_longitudes = np.linspace(np.min(longitudes), np.max(longitudes), num=67)\n",
    "        \n",
    "        # Create new meshgrid\n",
    "        new_lon_grid, new_lat_grid = np.meshgrid(new_longitudes, new_latitudes)\n",
    "        \n",
    "        new_vars = {}\n",
    "        \n",
    "        for var_name in ds_filtered.data_vars:\n",
    "            var = ds_filtered[var_name]\n",
    "            new_var_list = []\n",
    "            \n",
    "            for t in range(len(var.time)):\n",
    "                weather_variable = var.values[t, :, :]  # Shape (67, 71)\n",
    "                \n",
    "                # Flatten the weather variable data\n",
    "                values = weather_variable.flatten()\n",
    "                \n",
    "                # Interpolate the data onto the new grid\n",
    "                new_weather_variable = griddata(points, values, (new_lon_grid, new_lat_grid), method='linear')\n",
    "                \n",
    "                # Append the interpolated data for the current time step\n",
    "                new_var_list.append(new_weather_variable)\n",
    "            \n",
    "            # Stack the new variables along the time dimension\n",
    "            new_vars[var_name] = (['time', 'latitude', 'longitude'], np.stack(new_var_list))\n",
    "        \n",
    "        # Create a new xarray Dataset\n",
    "        new_ds = xr.Dataset(\n",
    "            new_vars, coords={'latitude': new_latitudes,\n",
    "                              'longitude': new_longitudes,\n",
    "                              'time': time.values})\n",
    "        \n",
    "        # Add the original variable attributes\n",
    "        new_ds['time'].attrs.update(time_attrs)\n",
    "        new_ds['latitude'].attrs.update(lat_attrs)\n",
    "        new_ds['longitude'].attrs.update(lon_attrs)\n",
    "\n",
    "        # Add global attributes\n",
    "        new_ds.attrs.update(ds_filtered.attrs)\n",
    "\n",
    "        #Saving files\n",
    "        filename = os.path.basename(file_name)\n",
    "        output_file = os.path.join(output_dir, filename)\n",
    "        print(f\"Saving file to: {output_file}\")  # Print the output file path\n",
    "        new_ds.to_netcdf(output_file)\n",
    "\n",
    "def main():\n",
    "    input_dir = \"/D4/data/gvaillant/NAM-2019-netcdf/03\"\n",
    "    output_dir = \"/D4/data/gvaillant/NAM/2019/wind-stage1/03\" #fix this directory need to create a new one\n",
    "    variables = {'UGRD_10maboveground': 'U10', 'VGRD_10maboveground': 'V10', 'PRES_surface': 'PSFC'}\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    \n",
    "    NAM_filter_and_match_dims(input_dir, output_dir, variables)\n",
    "    \n",
    "    print(\"Done processing stage1 NAM files!\")\n",
    "    \n",
    "#Uncomment below to run:\n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80581cbb-787a-4773-845a-62c363d28fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "def wind_speed(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Calculates wind speed from U10 and V10 components for NetCDF files in the input directory,\n",
    "    adds it as a new variable ('WS'), keeps only PSFC and WS, and saves the modified files to the output directory.\n",
    "\n",
    "    Parameters:\n",
    "        input_dir (str): Path to the directory containing input NetCDF files.\n",
    "        output_dir (str): Path to the directory to save the modified NetCDF files.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get list of all files in the input directory\n",
    "    input_files = glob.glob(os.path.join(input_dir, '*'))\n",
    "\n",
    "    for file_name in input_files:\n",
    "        print(f\"Processing file: {file_name}\")\n",
    "        try:\n",
    "            # Open the NetCDF file\n",
    "            ds = xr.open_dataset(file_name)\n",
    "\n",
    "            # Calculate wind speed\n",
    "            u = ds['U10'].data  # Extract data as numpy array\n",
    "            v = ds['V10'].data  # Extract data as numpy array\n",
    "            wind_speed = np.sqrt(u**2 + v**2)\n",
    "\n",
    "            # Add wind speed as a new variable to the dataset\n",
    "            ds['WS'] = (('time', 'latitude', 'longitude'), wind_speed)\n",
    "            ds['WS'].attrs['units'] = 'm/s'\n",
    "            ds['WS'].attrs['description'] = 'Calculated wind speed from U10 and V10'\n",
    "\n",
    "            # Keep only PSFC and WS\n",
    "            new_ds = ds[['PSFC', 'WS']]\n",
    "\n",
    "            # Save the modified dataset to the output directory\n",
    "            output_file_name = os.path.basename(file_name)\n",
    "            output_file_path = os.path.join(output_dir, output_file_name)\n",
    "            new_ds.to_netcdf(output_file_path)\n",
    "            print(f\"Saved file to: {output_file_path}\")\n",
    "\n",
    "            # Close the dataset\n",
    "            ds.close()\n",
    "            new_ds.close()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_name}: {e}\")\n",
    "\n",
    "def main():\n",
    "        input_dir = \"/D4/data/gvaillant/NAM/2019/wind-stage1/03\"\n",
    "        print(f\"Processing directory: {input_dir}\")\n",
    "\n",
    "        output_dir = \"/D4/data/gvaillant/NAM/2019/windspeed/03\"\n",
    "        print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "        wind_speed(input_dir, output_dir)\n",
    "        print(f\"Done processing directory: {input_dir}\")\n",
    "\n",
    "        print(\"Done adding wind speed variable to NAM files!\")\n",
    "    \n",
    "#Uncomment below to run:\n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee22ad2c-a7b1-4680-8e99-87af4ab9590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import os\n",
    "#need to get it cut down to around the same area as uWRF\n",
    "\n",
    "def NAM_spatial_filter(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Function to load the NAM dataset and spatially cut it to the uWRF bounds.\n",
    "    \n",
    "    Args:\n",
    "    - input_dir (str): Path to the NAM directory\n",
    "    - output_dir (str): Path to new directory to store filtered NAM data\n",
    "    Returns:\n",
    "    - None: Saves to the output directory.\n",
    "    \"\"\"\n",
    "    #Bounds to match up with uWRF (but a little higher)\n",
    "    #min_lat = 39.7\n",
    "    #max_lat = 43.2\n",
    "    #min_lon = -76.9\n",
    "    #max_lon = -72.5\n",
    "\n",
    "    #Bounds to match up with uWRF covering only NYC\n",
    "    min_lat = 40.380194091796874\n",
    "    max_lat = 41.090039825439455\n",
    "    min_lon = -74.34615478515624\n",
    "    max_lon = -73.495703125\n",
    "\n",
    "    for file_name in os.listdir(input_dir):\n",
    "        print(f\"Processing file: {file_name}\")\n",
    "        if file_name.endswith('.nc'):\n",
    "\n",
    "            file_path = os.path.join(input_dir, file_name)\n",
    "            dataset = xr.open_dataset(file_path)\n",
    "            \n",
    "            #Extract latitude and longitude variables\n",
    "            lat_var = 'latitude'\n",
    "            lon_var = 'longitude'\n",
    "            lat = dataset[lat_var].values\n",
    "            lon = dataset[lon_var].values\n",
    "            \n",
    "            #Filter the data based off of the spatial bounds\n",
    "            filtered_data = dataset.where(\n",
    "                (dataset[lat_var] >= min_lat) & (dataset[lat_var] <= max_lat) &\n",
    "                (dataset[lon_var] >= min_lon) & (dataset[lon_var] <= max_lon), drop=True)\n",
    "\n",
    "            output_file_path = os.path.join(output_dir, file_name)\n",
    "            filtered_data.to_netcdf(output_file_path)\n",
    "\n",
    "def main():\n",
    "    input_dir = \"/D4/data/gvaillant/NAM/2019/windspeed/03\"\n",
    "    output_dir = \"/D4/data/gvaillant/NAM/2019/wind-stage2/03\"\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    \n",
    "    NAM_spatial_filter(input_dir, output_dir)\n",
    "    \n",
    "    print(\"Done spatially filtering the NAM files!\")\n",
    "\n",
    "#Uncomment below to run:\n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07956c2b-ef99-4f94-bdc4-70b4b188867e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: /D4/data/gvaillant/NAM/2019/wind-final/train\n",
      "Saved combined dataset to: /D4/data/gvaillant/NAM/2019/wind-final/train/NAM_final_01-02.nc\n",
      "Done combining the first two months of NAM files!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "\n",
    "def NAM_combine_seq(input_dirs, output_dir):\n",
    "    # Given two directories, combine files sequentially\n",
    "    combined_datasets = []\n",
    "    \n",
    "    for input_dir in input_dirs:\n",
    "        nc_files = sorted([os.path.join(input_dir, file) for file in os.listdir(input_dir) if file.endswith('.nc')])\n",
    "        datasets = [xr.open_dataset(nc_file) for nc_file in nc_files]\n",
    "        combined_datasets.extend(datasets)  # Add datasets sequentially\n",
    "\n",
    "    # Concatenate along the time dimension\n",
    "    combined_dataset = xr.concat(combined_datasets, dim='time')\n",
    "    \n",
    "    # Generate the output filename based on the directories' month identifiers\n",
    "    month_range = \"-\".join([os.path.basename(dir) for dir in input_dirs])\n",
    "    output_file_name = f'NAM_final_{month_range}.nc'\n",
    "    output_file_path = os.path.join(output_dir, output_file_name)\n",
    "    \n",
    "    # Save the combined dataset\n",
    "    combined_dataset.to_netcdf(output_file_path, encoding={'time': {'units': 'hours since 2019-1-1'}})\n",
    "    print(f\"Saved combined dataset to: {output_file_path}\")\n",
    "\n",
    "def main():\n",
    "\n",
    "    input_dirs = [\n",
    "        \"/D4/data/gvaillant/NAM/2019/wind-stage2/01\",\n",
    "        \"/D4/data/gvaillant/NAM/2019/wind-stage2/02\"\n",
    "    ]\n",
    "    \n",
    "    output_dir = \"/D4/data/gvaillant/NAM/2019/wind-final/train\"\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    \n",
    "    NAM_combine_seq(input_dirs, output_dir)\n",
    "    \n",
    "    print(\"Done combining the first two months of NAM files!\")\n",
    "\n",
    "#Uncomment below to run:\n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0059e9b7-0e45-4e96-b0f7-1941b3821c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "\n",
    "def NAM_val_test(input_dirs, output_dir):\n",
    "    # Given two directories, combine files sequentially\n",
    "    combined_datasets = []\n",
    "    \n",
    "    for input_dir in input_dirs:\n",
    "        # Get sorted list of .nc files\n",
    "        nc_files = sorted([os.path.join(input_dir, file) for file in os.listdir(input_dir) if file.endswith('.nc')])\n",
    "        \n",
    "        # Take the first half of the files\n",
    "        half_length = len(nc_files) // 2\n",
    "        selected_files = nc_files[:half_length]  # Select first half of the sorted files (val)\n",
    "        #selected_files = nc_files[half_length:] #Select second half of the sorted files (test)\n",
    "        # Open the selected files\n",
    "        datasets = [xr.open_dataset(nc_file) for nc_file in selected_files]\n",
    "        combined_datasets.extend(datasets)  # Add datasets sequentially\n",
    "\n",
    "    # Concatenate along the time dimension\n",
    "    combined_dataset = xr.concat(combined_datasets, dim='time')\n",
    "    \n",
    "    # Generate the output filename based on the directories' month identifiers\n",
    "    month_range = \"-\".join([os.path.basename(dir) for dir in input_dirs])\n",
    "    output_file_name = f'NAM_final_{month_range}.nc'\n",
    "    output_file_path = os.path.join(output_dir, output_file_name)\n",
    "    \n",
    "    # Save the combined dataset\n",
    "    combined_dataset.to_netcdf(output_file_path, encoding={'time': {'units': 'hours since 2019-1-1'}})\n",
    "    print(f\"Saved combined dataset to: {output_file_path}\")\n",
    "\n",
    "def main():\n",
    "    input_dirs = [\n",
    "        \"/D4/data/gvaillant/NAM/2019/wind-stage2/03\"\n",
    "    ]\n",
    "    \n",
    "    output_dir = \"/D4/data/gvaillant/NAM/2019/match-NYC-final/val\"\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    \n",
    "    NAM_val_test(input_dirs, output_dir)\n",
    "    \n",
    "    print(\"Done combining the first half of the NAM files!\")\n",
    "\n",
    "#Uncomment below to run:\n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4903f78d-70c6-4ce7-b095-f81f8ef26540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL uWRF files for wind speed and surface pressure as a predictor\n",
    "\n",
    "uwrf_train_data = xr.open_dataset('/D4/data/gvaillant/prep-uwrf/d02/wind-NYC-split/train/uWRF_final_01-02.nc')\n",
    "uwrf_val_data = xr.open_dataset('/D4/data/gvaillant/prep-uwrf/d02/wind-NYC-split/val/uWRF_final_03.nc')\n",
    "uwrf_test_data = xr.open_dataset('/D4/data/gvaillant/prep-uwrf/d02/wind-NYC-split/test/uWRF_final_03.nc')\n",
    "\n",
    "# ---\n",
    "# NAM:\n",
    "nam_train_data = xr.open_dataset('/D4/data/gvaillant/NAM/2019/wind-final/train/NAM_final_01-02.nc')\n",
    "nam_val_data = xr.open_dataset('/D4/data/gvaillant/NAM/2019/match-NYC-final/val/NAM_final_03.nc')\n",
    "nam_test_data = xr.open_dataset('/D4/data/gvaillant/NAM/2019/match-NYC-final/test/NAM_final_03.nc')\n",
    "\n",
    "\n",
    "\n",
    "# Function to align datasets\n",
    "def align_datasets(uwrf_data, nam_data):\n",
    "    # uWRF grid dimensions\n",
    "    uwrf_shape = uwrf_data.WS.shape  # Assuming WS is representative of the shape\n",
    "    uwrf_lons = uwrf_data.longitude\n",
    "    uwrf_lats = uwrf_data.latitude\n",
    "\n",
    "    # Assign number of uWRF cells per NAM cell\n",
    "    uwrf_cells_per_lon = 4 #when using d03 we can use 12 bc we go from 12km to 1km\n",
    "    uwrf_cells_per_lat = 4\n",
    "\n",
    "    # Calculate new NAM grid dimensions\n",
    "    new_nam_lon_count = uwrf_shape[2] // uwrf_cells_per_lon\n",
    "    new_nam_lat_count = uwrf_shape[1] // uwrf_cells_per_lat\n",
    "\n",
    "    # Function to aggregate 4x4 uWRF cells into one NAM cell\n",
    "    def aggregate_4x4_grid(data):\n",
    "        reshaped = data.reshape(\n",
    "            data.shape[0],  # Time dimension remains unchanged\n",
    "            new_nam_lat_count, uwrf_cells_per_lat, \n",
    "            new_nam_lon_count, uwrf_cells_per_lon\n",
    "        )\n",
    "        aggregated = reshaped.mean(axis=(2, 4))  # Aggregate over latitude and longitude cells\n",
    "        return aggregated\n",
    "\n",
    "    # Determine the minimum time dimension between NAM and uWRF\n",
    "    min_time_steps = min(nam_data.time.size, uwrf_data.time.size)\n",
    "\n",
    "    # Slice both datasets to the same time dimension\n",
    "    nam_data_sliced = nam_data.isel(time=slice(0, min_time_steps))\n",
    "    uwrf_data_sliced = uwrf_data.isel(time=slice(0, min_time_steps))\n",
    "\n",
    "    # Initialize aligned data\n",
    "    aligned_data = {}\n",
    "\n",
    "    # Process both WS and PSFC\n",
    "    for var_name in ['WS', 'PSFC']:\n",
    "        if var_name in uwrf_data_sliced and var_name in nam_data_sliced:\n",
    "            uwrf_var = uwrf_data_sliced[var_name].values\n",
    "            aggregated_var = aggregate_4x4_grid(uwrf_var)\n",
    "            aligned_data[var_name] = (['time', 'latitude', 'longitude'], aggregated_var)\n",
    "        else:\n",
    "            raise ValueError(f\"Variable '{var_name}' not found in one of the datasets.\")\n",
    "\n",
    "    # Create a new dataset with aligned data\n",
    "    aligned_nam = xr.Dataset(\n",
    "        data_vars=aligned_data,\n",
    "        coords={\n",
    "            'time': nam_data_sliced.time,\n",
    "            'latitude': uwrf_lats[::uwrf_cells_per_lat][:new_nam_lat_count],\n",
    "            'longitude': uwrf_lons[::uwrf_cells_per_lon][:new_nam_lon_count]\n",
    "        },\n",
    "        attrs=nam_data.attrs\n",
    "    )\n",
    "\n",
    "    return aligned_nam\n",
    "\n",
    "#Uncomment the lines below to run the code:\n",
    "#aligned_nam_data = align_datasets(uwrf_val_data, nam_val_data)\n",
    "#aligned_nam_data.to_netcdf(\"/home/gvaillant1/wind-aligned-data/aligned_nam_val_data.nc\")\n",
    "#print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
