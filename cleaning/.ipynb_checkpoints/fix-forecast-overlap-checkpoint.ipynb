{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "833144de-182b-472e-8090-493029847124",
   "metadata": {},
   "source": [
    "# Fixing Time Duplication Problem (In progress)\n",
    "\n",
    "Instead of inputting one large sequential file into the model at once we instead input multiple, smaller, sequential files into the model. Currently, we input about 3 months of sequential training data into the model, but now we will input each day at a time into the model for the same 3 months. Each day file will consist of 84 hours. We try this approach to avoid the time duplication problem we experience when combining all the days together into one file, because there are overlapping forecasts.\n",
    "\n",
    "\n",
    "Day files are now found in /D4/data/gvaillant/prep-uwrf/d02/day_by_day\n",
    "\n",
    "TO DO:\n",
    "\n",
    "\n",
    "In the jupyter notebooks that contain each model, we need to apply the scaling, adding channel dim steps to each file in that directory. I think we can create another new directory named 'uwrf_train'. Then all the data will be ready for the model.\n",
    "\n",
    "For the 'data_train' argument in the model, we need to replace uwrf_train with something like [for file in uwrf_train] so there is a list of data files the model will interate through. Adjust the batch size accordingly... maybe 84 for the 84 forecast hours?\n",
    "\n",
    "Dimensions for each NAM training file:\n",
    "* time = 29 (84 total forecast hours, every 3 hours)\n",
    "* lat = 5\n",
    "* lon = 5\n",
    "* channel = 1\n",
    "\n",
    "\n",
    "This method allows us to save all the 84 forecast hours, but also avoid the time duplication ... \n",
    "I guess each training dataset can be thought of day files OR forecast intilizations, since there is one model initialization per day\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2bf36b-d07c-438b-86cf-6eaa02d67c41",
   "metadata": {},
   "source": [
    "# Reorganizing uWRF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb994eef-da3b-4199-85a1-9205b6848636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "#Correct code for making the dummy filler files for the missing ones!\n",
    "def create_netcdf_for_missing_day(reference_file_path, missing_day_dir_path, start_date, duration_hours=84):\n",
    "    \"\"\"\n",
    "    Create NetCDF files for a missing day with timestamps every 3 hours.\n",
    "    \n",
    "    Parameters:\n",
    "    - reference_file_path: Path to the reference NetCDF file\n",
    "    - missing_day_dir_path: Directory path for the missing day\n",
    "    - start_date: Start date of the missing period\n",
    "    - duration_hours: Total duration of files (default 84 hours)\n",
    "    \"\"\"\n",
    "    # Ensure the missing day directory exists\n",
    "    os.makedirs(missing_day_dir_path, exist_ok=True)\n",
    "    \n",
    "    # Check if directory is empty\n",
    "    input_files = sorted(glob.glob(os.path.join(missing_day_dir_path, 'wrfout_d02_*')))\n",
    "    if len(input_files) > 0:\n",
    "        print(f\"Directory {missing_day_dir_path} is not empty. Skipping.\")\n",
    "        return\n",
    "    \n",
    "    # Open reference dataset\n",
    "    ds_ref = xr.open_dataset(reference_file_path)\n",
    "    \n",
    "    # Extract latitude and longitude\n",
    "    lat = ds_ref['latitude'].values\n",
    "    lon = ds_ref['longitude'].values\n",
    "    \n",
    "    # Extract reference variable data (first timestep)\n",
    "    T2_data = ds_ref['T2'].isel(time=0).values\n",
    "    PSFC_data = ds_ref['PSFC'].isel(time=0).values\n",
    "    \n",
    "    # Ensure data shapes match coordinates\n",
    "    T2_data = T2_data[:len(lat), :len(lon)]\n",
    "    PSFC_data = PSFC_data[:len(lat), :len(lon)]\n",
    "    \n",
    "    # Generate timestamps every 3 hours\n",
    "    current_date = datetime.strptime(start_date, \"%Y-%m-%dT%H:%M:%S\")\n",
    "    end_date = current_date + timedelta(hours=duration_hours)\n",
    "    \n",
    "    while current_date <= end_date:\n",
    "        timestamp = np.datetime64(current_date)\n",
    "        \n",
    "        # Create output path\n",
    "        output_path = os.path.join(missing_day_dir_path, f'wrfout_d02_{timestamp.astype(datetime).strftime(\"%Y-%m-%d_%H:%M:%S\")}')\n",
    "        \n",
    "        # Create a new dataset\n",
    "        ds = xr.Dataset(\n",
    "            {\n",
    "                \"T2\": ([\"latitude\", \"longitude\"], T2_data),\n",
    "                \"PSFC\": ([\"latitude\", \"longitude\"], PSFC_data),\n",
    "            },\n",
    "            coords={\n",
    "                \"latitude\": lat,\n",
    "                \"longitude\": lon,\n",
    "                \"time\": [timestamp]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Copy attributes from reference dataset\n",
    "        ds.attrs = ds_ref.attrs\n",
    "        \n",
    "        # Save the new NetCDF file\n",
    "        ds.to_netcdf(output_path)\n",
    "        print(f\"Created file: {output_path}\")\n",
    "        \n",
    "        # Increment by 3 hours\n",
    "        current_date += timedelta(hours=3)\n",
    "    \n",
    "    # Close the reference dataset\n",
    "    ds_ref.close()\n",
    "\n",
    "def generate_missing_files():\n",
    "    # January missing days\n",
    "    january_missing_days = [\n",
    "        {'dir': '/D4/data/gvaillant/prep-uwrf/d02/pred-stage3/01/03', 'ref': '/D4/data/gvaillant/prep-uwrf/d02/pred-stage3/01/02/wrfout_d02_2019-01-02_00:00:00', 'start': '2019-01-03T00:00:00'},\n",
    "        {'dir': '/D4/data/gvaillant/prep-uwrf/d02/pred-stage3/01/04', 'ref': '/D4/data/gvaillant/prep-uwrf/d02/pred-stage3/01/02/wrfout_d02_2019-01-02_00:00:00', 'start': '2019-01-04T00:00:00'},\n",
    "        {'dir': '/D4/data/gvaillant/prep-uwrf/d02/pred-stage3/01/05', 'ref': '/D4/data/gvaillant/prep-uwrf/d02/pred-stage3/01/02/wrfout_d02_2019-01-02_00:00:00', 'start': '2019-01-05T00:00:00'},\n",
    "        {'dir': '/D4/data/gvaillant/prep-uwrf/d02/pred-stage3/01/06', 'ref': '/D4/data/gvaillant/prep-uwrf/d02/pred-stage3/01/02/wrfout_d02_2019-01-02_00:00:00', 'start': '2019-01-06T00:00:00'},\n",
    "        {'dir': '/D4/data/gvaillant/prep-uwrf/d02/pred-stage3/01/07', 'ref': '/D4/data/gvaillant/prep-uwrf/d02/pred-stage3/01/02/wrfout_d02_2019-01-02_00:00:00', 'start': '2019-01-07T00:00:00'}\n",
    "    ]\n",
    "    \n",
    "    # February missing day\n",
    "    february_missing_day = [\n",
    "        {'dir': '/D4/data/gvaillant/prep-uwrf/d02/pred-stage3/02/06', 'ref': '/D4/data/gvaillant/prep-uwrf/d02/pred-stage3/02/05/wrfout_d02_2019-02-05_00:00:00', 'start': '2019-02-06T00:00:00'}\n",
    "    ]\n",
    "    \n",
    "    # Combine and process all missing days\n",
    "    all_missing_days = january_missing_days + february_missing_day\n",
    "    \n",
    "    for day_info in all_missing_days:\n",
    "        create_netcdf_for_missing_day(\n",
    "            reference_file_path=day_info['ref'], \n",
    "            missing_day_dir_path=day_info['dir'], \n",
    "            start_date=day_info['start']\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_missing_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d08ab1-9d76-4acd-9cd5-40da1588999b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "output_dir = \"/D4/data/gvaillant/prep-uwrf/d02/day_by_day/02\" # **CHANGE DEPENDING ON MONTH**\n",
    "\n",
    "for i in range(1, 29):\n",
    "\n",
    "    #Take each day directory in the original directory\n",
    "    day_dir_path = f\"/D4/data/gvaillant/prep-uwrf/d02/pred-stage3/02/{str(i).zfill(2)}\" # ** CHANGE **\n",
    "    input_files = sorted(glob.glob(os.path.join(day_dir_path, 'wrfout_d02_*')))  # Gather all files in the daily directory\n",
    "    \n",
    "    if not input_files:\n",
    "        print(f\"No files found for day {i} in directory {day_dir_path} Skipping...\")\n",
    "        continue  # Skip the current day if no files are found\n",
    "    \n",
    "    # Print the files being found (debugging step)\n",
    "    print(f\"Files found for day {i}: {input_files}\")\n",
    "    #Load each file into a dataset and add to the list\n",
    "    datasets = [xr.open_dataset(file) for file in input_files]\n",
    "\n",
    "    #Combine datasets along the time dimension\n",
    "    combined_dataset = xr.concat(datasets, dim='time')\n",
    "    \n",
    "    #Now all the forecasts that made up one day, are combined into one file!\n",
    "\n",
    "    #Construct the output file name\n",
    "    output_file_name = f'uwrf_2019_02_{str(i).zfill(2)}.nc' # ** CHANGE **\n",
    "    output_file_path = os.path.join(output_dir, output_file_name) \n",
    "    \n",
    "    time_origin = f\"hours since 2019-02-{str(i).zfill(2)} 00:00:00\" # ** CHANGE **\n",
    "\n",
    "    combined_dataset.to_netcdf(output_file_path, encoding={'time': {'units': time_origin}})\n",
    "    \n",
    "    print(f'Combined dataset for day {i} saved to {output_file_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6568a78d-fda9-4829-931c-66515f495449",
   "metadata": {},
   "source": [
    "# Now that we have the uWRF data in the correct squential format, we need to dictate which files are for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce63445e-969a-4e3f-8a33-b470b837e9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "# Define the input directories and the output directory\n",
    "input_dirs = [\n",
    "    '/D4/data/gvaillant/prep-uwrf/d02/day_by_day/01',\n",
    "    '/D4/data/gvaillant/prep-uwrf/d02/day_by_day/02'\n",
    "]\n",
    "output_dir = '/D4/data/gvaillant/prep-uwrf/d02/day-final/train'\n",
    "\n",
    "\n",
    "# Iterate over input directories and move files to the output directory\n",
    "for input_dir in input_dirs:\n",
    "    # Get sorted list of files in the input directory\n",
    "    files = sorted(glob.glob(os.path.join(input_dir, '*')))\n",
    "    \n",
    "    for file in files:\n",
    "        # Extract the filename to avoid overwriting files with the same name\n",
    "        filename = os.path.basename(file)\n",
    "        target_path = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Copy the file to the output directory\n",
    "        shutil.copy(file, target_path)\n",
    "        print(f\"Copied: {file} -> {target_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882a1b36-9aea-4c1a-b471-b8194e7903a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "# Define the input directories and the output directory\n",
    "input_dirs = [\n",
    "    '/D4/data/gvaillant/prep-uwrf/d02/day_by_day/03'\n",
    "]\n",
    "\n",
    "output_dir = '/D4/data/gvaillant/prep-uwrf/d02/day-final/val'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Iterate over input directories and copy files to the output directory\n",
    "for input_dir in input_dirs:\n",
    "    # Get sorted list of files in the input directory\n",
    "    files = sorted(glob.glob(os.path.join(input_dir, '*')))\n",
    "\n",
    "    # First half of the March files will be val\n",
    "    half_length = len(files) // 2\n",
    "    selected_files = files[:half_length]\n",
    "    \n",
    "    for file in selected_files:\n",
    "        # Extract the filename to avoid overwriting files with the same name\n",
    "        filename = os.path.basename(file)\n",
    "        target_path = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Copy the file to the output directory\n",
    "        shutil.copy(file, target_path)\n",
    "        print(f\"Copied: {file} -> {target_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab436aa-4fc3-4b09-8431-e1b96d686ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "# Define the input directories and the output directory\n",
    "input_dirs = [\n",
    "    '/D4/data/gvaillant/prep-uwrf/d02/day_by_day/03'\n",
    "]\n",
    "\n",
    "output_dir = '/D4/data/gvaillant/prep-uwrf/d02/day-final/test'\n",
    "\n",
    "\n",
    "# Iterate over input directories and move files to the output directory\n",
    "for input_dir in input_dirs:\n",
    "    # Get sorted list of files in the input directory\n",
    "    files = sorted(glob.glob(os.path.join(input_dir, '*')))\n",
    "\n",
    "    #Second half of the march files will be test\n",
    "    half_length = len(files) // 2\n",
    "    selected_files = files[half_length:]\n",
    "    \n",
    "    for file in selected_files:\n",
    "        # Extract the filename to avoid overwriting files with the same name\n",
    "        filename = os.path.basename(file)\n",
    "        target_path = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Copy the file to the output directory\n",
    "        shutil.copy(file, target_path)\n",
    "        print(f\"Copied: {file} -> {target_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02109022-6f78-4e54-b4b5-564b3bab452b",
   "metadata": {},
   "source": [
    "# Reorganizing NAM files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02b1a49a-5517-4c9e-a06c-f69cc33ccbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Files to Create:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "## check for missing files for each month and then create them..\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "source_dir = '/D4/data/gvaillant/NAM/2019/match-NYC-cut/03'\n",
    "output_dir = '/D4/data/gvaillant/NAM/2019/match-NYC-cut/03'\n",
    "\n",
    "# Sample file pattern\n",
    "file_pattern = re.compile(r\"domnys-nam_218_(\\d{8})_(\\d{4})_0(\\d{2})\\.nc\")  # Capture date, time, and sequence number\n",
    "\n",
    "# Organize files by date based on the filename\n",
    "files_by_date = {}\n",
    "for filename in os.listdir(source_dir):\n",
    "    if filename.endswith('.nc'):\n",
    "        match = file_pattern.match(filename)\n",
    "        if match:\n",
    "            date = match.group(1)  # Extract the date (YYYYMMDD)\n",
    "            time = match.group(3)  # Extract the hour (HHH)\n",
    "            files_by_date.setdefault(date, []).append(time)  # Store just the time (HHMM) for each file\n",
    "\n",
    "# Check for dates with missing files (assuming we expect 29 files)\n",
    "date_with_missing_files = []\n",
    "for date, times in files_by_date.items():\n",
    "    if len(times) != 29:\n",
    "        date_with_missing_files.append(date)\n",
    "        print(f\"{date} does not have all files\")\n",
    "\n",
    "# List of reference hours (0, 3, 6, ..., 81)\n",
    "reference_hours = [f\"{h:02d}\" for h in range(0, 85, 3)]  # Expected hours: '00', '03', ..., '81'\n",
    "\n",
    "# List to hold the actual missing hours for each date\n",
    "actual_missing_files = []\n",
    "\n",
    "# Find the missing hours for each date\n",
    "for date in date_with_missing_files:\n",
    "    # Extract all hours (first two digits of the time, i.e., 'HH' from 'HHMM')\n",
    "    present_hours = [time[:2] for time in files_by_date[date]]  # Get the 'HH' part of the time\n",
    "    \n",
    "    # Check for missing hours\n",
    "    missing_hours = [hour for hour in reference_hours if hour not in present_hours]\n",
    "    \n",
    "    if missing_hours:\n",
    "        actual_missing_files.append((date, missing_hours))\n",
    "\n",
    "print(\"Missing Files to Create:\")\n",
    "print(actual_missing_files)\n",
    "\n",
    "\n",
    "def get_timestamp(date, hour):\n",
    "    # Assuming date is in YYYYMMDD format and hour is in two-digit format\n",
    "    datetime_str = f\"{date[:4]}-{date[4:6]}-{date[6:]} {hour}:00:00\"\n",
    "    timestamp = pd.to_datetime(datetime_str)\n",
    "    return timestamp\n",
    "\n",
    "def create_missing_files(missing_data, source_dir, output_dir, reference_file):\n",
    "\n",
    "    # Open the reference dataset\n",
    "    ds_ref = xr.open_dataset(reference_file)\n",
    "\n",
    "    # Extract reference data\n",
    "    lat = ds_ref['latitude'].values\n",
    "    lon = ds_ref['longitude'].values\n",
    "\n",
    "    for date, hours in missing_data:\n",
    "        for hour in hours:\n",
    "            # Generate the missing filename\n",
    "            filename = f\"domnys-nam_218_{date}_0000_0{hour}.nc\"\n",
    "            output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "            # Adjust the hour and date for timestamps exceeding 24\n",
    "            base_date = pd.to_datetime(date, format='%Y%m%d')\n",
    "            hour = int(hour)\n",
    "            day_offset = hour // 24  # Calculate the number of extra days\n",
    "            adjusted_hour = hour % 24  # Get the hour within the day\n",
    "            adjusted_date = base_date + pd.Timedelta(days=day_offset)  # Add offset days\n",
    "            timestamp = adjusted_date + pd.Timedelta(hours=adjusted_hour)  # Final timestamp\n",
    "\n",
    "            # Extract reference variable data\n",
    "            T2_data = ds_ref['T2'].isel(time=0).values\n",
    "            PRES_data = ds_ref['PRES'].isel(time=0).values\n",
    "\n",
    "            # Ensure data shapes match coordinates\n",
    "            T2_data = T2_data[:len(lat), :len(lon)]  # Adjust T2 data shape\n",
    "            PRES_data = PRES_data[:len(lat), :len(lon)]  # Adjust PRES data shape\n",
    "\n",
    "            # Create a new dataset\n",
    "            ds = xr.Dataset(\n",
    "                {\n",
    "                    \"T2\": ([\"latitude\", \"longitude\"], T2_data),\n",
    "                    \"PRES\": ([\"latitude\", \"longitude\"], PRES_data),\n",
    "                },\n",
    "                coords={\n",
    "                    \"latitude\": lat,\n",
    "                    \"longitude\": lon,\n",
    "                    \"time\": [timestamp]\n",
    "                },\n",
    "            )\n",
    "\n",
    "            # Save the dataset to a NetCDF file\n",
    "            ds.to_netcdf(output_path)\n",
    "            print(f\"Created file: {output_path} with timestamp {timestamp}\")\n",
    "\n",
    "\n",
    "\n",
    "#I dont want to have to choose the reference file everytime..\n",
    "#fix this method later on..\n",
    "reference_file = '/D4/data/gvaillant/NAM/2019/match-NYC-cut/03/domnys-nam_218_20190301_0000_000.nc' \n",
    "\n",
    "create_missing_files(actual_missing_files, source_dir, output_dir, reference_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891c7b62-2f28-4346-acfd-7638590d7718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import re\n",
    "\n",
    "# Define directories\n",
    "source_dir = '/D4/data/gvaillant/NAM/2019/match-NYC-cut/03' # ** CHANGE **\n",
    "target_dir = '/D4/data/gvaillant/NAM/2019/day-by-day/03' # ** CHANGE **\n",
    "\n",
    "# File naming pattern to extract the date\n",
    "file_pattern = re.compile(r\"domnys-nam_218_(\\d{8})_\\d{4}_\\d{3}\\.nc\")\n",
    "\n",
    "# Organize files by date based on the filename\n",
    "files_by_date = {}\n",
    "for filename in os.listdir(source_dir):\n",
    "    if filename.endswith('.nc'):\n",
    "        match = file_pattern.match(filename)\n",
    "        if match:\n",
    "            date = match.group(1)  # Extract the date (YYYYMMDD)\n",
    "            files_by_date.setdefault(date, []).append(os.path.join(source_dir, filename))\n",
    "\n",
    "# Process files for each date\n",
    "for date, file_list in files_by_date.items():\n",
    "    print(f\"Processing {len(file_list)} files for date {date}...\")\n",
    "    \n",
    "    # Open and combine datasets for the specific date\n",
    "    datasets = [xr.open_dataset(filepath) for filepath in sorted(file_list)]\n",
    "    \n",
    "    # Combine datasets along the time dimension, ensuring no artificial time creation\n",
    "    combined_dataset = xr.concat(datasets, dim=\"time\")\n",
    "    \n",
    "    # Close individual datasets after concatenation\n",
    "    for ds in datasets:\n",
    "        ds.close()\n",
    "\n",
    "    # Save the combined dataset for this date\n",
    "    output_file = os.path.join(target_dir, f\"domnys-nam_218_{date}.nc\")\n",
    "    combined_dataset.to_netcdf(output_file, encoding={'time': {'dtype': 'float64'}})\n",
    "    print(f\"Saved combined dataset to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18afb73a-2322-4c43-ab1a-fd3aa30db492",
   "metadata": {},
   "source": [
    "# MAKE INTO TRAIN TEST AND VAL BEFORE REGRIDDING ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e4a3e8-f3c9-4885-b920-f86dbd5f0acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "# Define the input directories and the output directory\n",
    "input_dirs = [\n",
    "    '/D4/data/gvaillant/NAM/2019/day-by-day/01',\n",
    "    '/D4/data/gvaillant/NAM/2019/day-by-day/02'\n",
    "]\n",
    "output_dir = '/D4/data/gvaillant/NAM/2019/day-final/train'\n",
    "\n",
    "\n",
    "# Iterate over input directories and move files to the output directory\n",
    "for input_dir in input_dirs:\n",
    "    # Get sorted list of files in the input directory\n",
    "    files = sorted(glob.glob(os.path.join(input_dir, '*')))\n",
    "    \n",
    "    for file in files:\n",
    "        # Extract the filename to avoid overwriting files with the same name\n",
    "        filename = os.path.basename(file)\n",
    "        target_path = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Copy the file to the output directory\n",
    "        shutil.copy(file, target_path)\n",
    "        print(f\"Copied: {file} -> {target_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598af839-c20a-4678-9757-cd4b4743168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "# Define the input directories and the output directory\n",
    "input_dirs = [\n",
    "    '/D4/data/gvaillant/NAM/2019/day-by-day/03'\n",
    "]\n",
    "\n",
    "output_dir = '/D4/data/gvaillant/NAM/2019/day-final/val'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Iterate over input directories and copy files to the output directory\n",
    "for input_dir in input_dirs:\n",
    "    # Get sorted list of files in the input directory\n",
    "    files = sorted(glob.glob(os.path.join(input_dir, '*')))\n",
    "\n",
    "    # First half of the March files will be val\n",
    "    half_length = len(files) // 2\n",
    "    selected_files = files[:half_length]\n",
    "    \n",
    "    for file in selected_files:\n",
    "        # Extract the filename to avoid overwriting files with the same name\n",
    "        filename = os.path.basename(file)\n",
    "        target_path = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Copy the file to the output directory\n",
    "        shutil.copy(file, target_path)\n",
    "        print(f\"Copied: {file} -> {target_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26307b7-6213-4f71-b96d-38c0970257e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "\n",
    "input_dirs = ['/D4/data/gvaillant/NAM/2019/day-by-day/03']\n",
    "\n",
    "output_dir = '/D4/data/gvaillant/NAM/2019/day-final/test'\n",
    "\n",
    "\n",
    "# Iterate over input directories and move files to the output directory\n",
    "for input_dir in input_dirs:\n",
    "    # Get sorted list of files in the input directory\n",
    "    files = sorted(glob.glob(os.path.join(input_dir, '*')))\n",
    "\n",
    "    #Second half of the march files will be test\n",
    "    half_length = len(files) // 2\n",
    "    selected_files = files[half_length:]\n",
    "    \n",
    "    for file in selected_files:\n",
    "        # Extract the filename to avoid overwriting files with the same name\n",
    "        filename = os.path.basename(file)\n",
    "        target_path = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Copy the file to the output directory\n",
    "        shutil.copy(file, target_path)\n",
    "        print(f\"Copied: {file} -> {target_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec3f049-4641-436e-a8db-d10bb78b19ea",
   "metadata": {},
   "source": [
    "# Apply regridding to NAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e19286-d2d7-462b-8586-0ae247a8af16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALMOST CORRECT BUT NEED TO FIX NUMBER OF UWRF FILES BC SOME MISSING\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "def align_datasets(uwrf_data, nam_data):\n",
    "    # uWRF grid dimensions\n",
    "    uwrf_shape = uwrf_data.T2.shape\n",
    "    uwrf_lons = uwrf_data.longitude\n",
    "    uwrf_lats = uwrf_data.latitude\n",
    "\n",
    "    # Assign number of uWRF cells per NAM cell\n",
    "    uwrf_cells_per_lon = 4 \n",
    "    uwrf_cells_per_lat = 4\n",
    "\n",
    "    # Calculate new NAM grid dimensions\n",
    "    new_nam_lon_count = uwrf_shape[2] // uwrf_cells_per_lon\n",
    "    new_nam_lat_count = uwrf_shape[1] // uwrf_cells_per_lat\n",
    "\n",
    "    # Function to aggregate 4x4 uWRF cells into one NAM cell\n",
    "    def aggregate_4x4_grid(data):\n",
    "        reshaped = data.reshape(\n",
    "            data.shape[0],  # Time dimension remains unchanged\n",
    "            new_nam_lat_count, uwrf_cells_per_lat,\n",
    "            new_nam_lon_count, uwrf_cells_per_lon\n",
    "        )\n",
    "        aggregated = reshaped.mean(axis=(2, 4))  # Aggregate over latitude and longitude cells\n",
    "        return aggregated\n",
    "\n",
    "    # Determine the minimum time dimension between NAM and uWRF\n",
    "    min_time_steps = min(nam_data.time.size, uwrf_data.time.size)\n",
    "\n",
    "    # Slice both datasets to the same time dimension\n",
    "    nam_data_sliced = nam_data.isel(time=slice(0, min_time_steps))\n",
    "    uwrf_data_sliced = uwrf_data.isel(time=slice(0, min_time_steps))\n",
    "\n",
    "    # Initialize aligned data\n",
    "    aligned_data = {}\n",
    "\n",
    "    # Process both T2 and PSFC\n",
    "    for var_name in ['T2', 'PSFC']:\n",
    "        if var_name in uwrf_data_sliced and var_name in nam_data_sliced:\n",
    "            uwrf_var = uwrf_data_sliced[var_name].values\n",
    "            aggregated_var = aggregate_4x4_grid(uwrf_var)\n",
    "            aligned_data[var_name] = (['time', 'latitude', 'longitude'], aggregated_var)\n",
    "        else:\n",
    "            raise ValueError(f\"Variable '{var_name}' not found in one of the datasets.\")\n",
    "\n",
    "    # Create a new dataset with aligned data\n",
    "    aligned_nam = xr.Dataset(\n",
    "        data_vars=aligned_data,\n",
    "        coords={\n",
    "            'time': nam_data_sliced.time,\n",
    "            'latitude': uwrf_lats[::uwrf_cells_per_lat][:new_nam_lat_count],\n",
    "            'longitude': uwrf_lons[::uwrf_cells_per_lon][:new_nam_lon_count]\n",
    "        },\n",
    "        attrs=nam_data.attrs\n",
    "    )\n",
    "\n",
    "    return aligned_nam\n",
    "\n",
    "\n",
    "\n",
    "def process_and_save(uwrf_dir, nam_dir, save_dir, dataset_type):\n",
    "    uwrf_files = sorted(glob.glob(os.path.join(uwrf_dir, '*')))\n",
    "    nam_files = sorted(glob.glob(os.path.join(nam_dir, '*')))\n",
    "\n",
    "    #print(len(uwrf_files), len(nam_files))\n",
    "\n",
    "    \n",
    "    for uwrf_file, nam_file in zip(uwrf_files, nam_files):\n",
    "        uwrf_data = xr.open_dataset(uwrf_file)\n",
    "        nam_data = xr.open_dataset(nam_file).rename({'PRES': 'PSFC'})\n",
    "\n",
    "        aligned_nam = align_datasets(uwrf_data, nam_data)\n",
    "        output_file = os.path.join(save_dir, f\"{os.path.basename(nam_file)}\")\n",
    "        aligned_nam.to_netcdf(output_file)\n",
    "        print(f\"Saved aligned dataset: {output_file}\")\n",
    "\n",
    "\n",
    "\n",
    "process_and_save(\n",
    "    uwrf_dir='/D4/data/gvaillant/prep-uwrf/d02/day-final/train',\n",
    "    nam_dir='/D4/data/gvaillant/NAM/2019/day-final/train',\n",
    "    save_dir='/D4/data/gvaillant/NAM/2019/aligned/train',\n",
    "    dataset_type='train'\n",
    ")\n",
    "\n",
    "process_and_save(\n",
    "    uwrf_dir='/D4/data/gvaillant/prep-uwrf/d02/day-final/val',\n",
    "    nam_dir='/D4/data/gvaillant/NAM/2019/day-final/val',\n",
    "    save_dir='/D4/data/gvaillant/NAM/2019/aligned/val',\n",
    "    dataset_type='val'\n",
    ")\n",
    "\n",
    "process_and_save(\n",
    "    uwrf_dir='/D4/data/gvaillant/prep-uwrf/d02/day-final/test',\n",
    "    nam_dir='/D4/data/gvaillant/NAM/2019/day-final/test',\n",
    "    save_dir='/D4/data/gvaillant/NAM/2019/aligned/test',\n",
    "    dataset_type='test'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
